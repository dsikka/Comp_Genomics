{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "supervised_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPKwiut19de2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import collections\n",
        "import torch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import MiniBatchSparsePCA\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, GridSearchCV, StratifiedKFold, cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8T2GP6V74nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0d16263-b9fa-4acb-83f0-33540b851b2d"
      },
      "source": [
        "!pip install mord"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mord in /usr/local/lib/python3.6/dist-packages (0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOco4wxe99gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mord as m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpDU8JH9de_",
        "colab_type": "text"
      },
      "source": [
        "## Load processed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foe7nuZh9dfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('train_data_processed.csv')\n",
        "test_data = pd.read_csv('test_data_processed.csv')\n",
        "train_labels = np.load('train_labels.csv.npy', allow_pickle=True)\n",
        "test_labels = np.load('test_labels.csv.npy', allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1MBW0tU9dfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explains more than 80% of the variance in the data\n",
        "pca = PCA(n_components=100)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_scaled = scaler.fit_transform(train_data)\n",
        "pca_train_genes = pca.fit_transform(train_scaled)\n",
        "\n",
        "test_scaled = scaler.transform(test_data)\n",
        "pca_test_genes = pca.transform(test_scaled)\n",
        "\n",
        "all_weights = pd.DataFrame(pca.components_)\n",
        "all_weights.columns = train_data.columns.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjHWgRpQ9dfP",
        "colab_type": "text"
      },
      "source": [
        "### Figure out what this is doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqn9OBlS9dfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_genes = []\n",
        "for i in range(50):\n",
        "    important_genes.append([pd.DataFrame(np.abs(all_weights)).T.nlargest(100, i).index.values])\n",
        "important_genes = np.unique(important_genes, return_counts=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAtRAAjQ9dfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_train = train_data[important_genes]\n",
        "important_test = test_data[important_genes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQRiH6yg9dfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_mapping = {'WHO II': 0, 'WHO III': 1, 'WHO IV': 2}\n",
        "reverse_mapping = {0: 'WHO II', 1: 'WHO III', 2: 'WHO IV'}\n",
        "mapped_labels_train = [label_mapping[x[0]] for x in train_labels]\n",
        "mapped_labels_test = [label_mapping[x[0]] for x in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcB56gkY9dff",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the important genes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOMQkTaW9dfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_pca(input_data, labels, title):\n",
        "    high_scaler = StandardScaler()\n",
        "    input_scaled = high_scaler.fit_transform(input_data)\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    transformed_data = pca.fit_transform(input_scaled)\n",
        "    \n",
        "    data_df = pd.DataFrame(data = transformed_data, columns = ['principal component 1', 'principal component 2'])\n",
        "    class_labels_df = pd.DataFrame(labels)\n",
        "    data_df = pd.concat([data_df, class_labels_df], axis = 1)\n",
        "    data_df.columns = ['PC1', 'PC2', 'Grade']\n",
        "    \n",
        "    fig = plt.figure(figsize = (20, 10))\n",
        "    ax = fig.add_subplot(1,2,1) \n",
        "    ax.set_title(title, fontsize = 15)\n",
        "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "    targets = ['WHO II', 'WHO III', 'WHO IV']\n",
        "    colors = ['r', 'g', 'b']\n",
        "    for target, color in zip(targets,colors):\n",
        "        indicesToKeep = data_df['Grade'] == target\n",
        "        ax.scatter(data_df.loc[indicesToKeep, 'PC1']\n",
        "                   , data_df.loc[indicesToKeep, 'PC2']\n",
        "                   , c = color\n",
        "                   , s = 50)\n",
        "    ax.legend(targets)\n",
        "    ax.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5-HmFb99dfj",
        "colab_type": "code",
        "outputId": "adedc05d-4f12-4bf0-f984-06c1fbdbc890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "source": [
        "display_pca(important_train, train_labels, 'Training Data: Using 2 Principal Components')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv-Y080X9dfq",
        "colab_type": "text"
      },
      "source": [
        "## Standard Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Axth1ZH9dfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 15)]\n",
        "max_depth = [int(x) for x in np.linspace(start = 3, stop = 20, num = 15)]\n",
        "min_samples_split = [int(x) for x in np.linspace(start = 1, stop = 50, num = 15)]\n",
        "rf = RandomForestClassifier(bootstrap='True', random_state=0, max_features='auto')\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvZ5VDZu9dfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_params = {'n_estimators': n_estimators, \n",
        "             'max_depth': max_depth, \n",
        "             'min_samples_split': min_samples_split}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z0iXfMU9dfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_random_search = RandomizedSearchCV(rf, rf_params, cv=cv_strategy, n_jobs=-1, return_train_score=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyImZ2_l32JG",
        "colab_type": "text"
      },
      "source": [
        "- Conducted on the 3000+ most important genes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAHJ48O_9df2",
        "colab_type": "code",
        "outputId": "011fd8c3-b7b7-44eb-a1a9-5e5fde455551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rf_random_search.fit(important_train, mapped_labels_train)\n",
        "rf_random_search.best_score_"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.688735632183908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VLYBT9r9df_",
        "colab_type": "code",
        "outputId": "1f052b4d-1cad-4247-9307-6efcd241b43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rf_random_search.score(important_test, mapped_labels_test)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7CfmFy734Tr",
        "colab_type": "text"
      },
      "source": [
        "- Conducted on the top 100 PCs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGRFVno64CZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dfa8de2-b365-4e8c-e18a-7efed939b895"
      },
      "source": [
        "rf_random_search_pca = RandomizedSearchCV(rf, rf_params, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "rf_random_search_pca.fit(pca_train_genes, mapped_labels_train)\n",
        "rf_random_search_pca.best_score_"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.661264367816092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1bLSFnc47GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "145d7eec-27c4-4e29-b64b-8682650fdf01"
      },
      "source": [
        "rf_random_search_pca.score(pca_test_genes, mapped_labels_test)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.696969696969697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa0swEXs9dgO",
        "colab_type": "text"
      },
      "source": [
        "# Kernel SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUTW6Cgb9dgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_train_scaled = scaler.fit_transform(important_train)\n",
        "important_test_scaled = scaler.transform(important_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTYZ_lUw56K5",
        "colab_type": "text"
      },
      "source": [
        "- Using 3000+ genes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UMhpSwQ9dgS",
        "colab_type": "code",
        "outputId": "d96ea563-33be-43c4-c0fa-17d77804d3d1",
        "colab": {}
      },
      "source": [
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C': np.logspace(-3, 3, 7), 'gamma': ('auto', 'scale')}\n",
        "svc_grid = GridSearchCV(SVC(), parameters, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "svc_grid.fit(important_train_scaled, mapped_labels_train)\n",
        "svc_grid.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6855172413793104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo593V89dgV",
        "colab_type": "code",
        "outputId": "f7440c51-9f58-4532-f2e2-a064e4f491a5",
        "colab": {}
      },
      "source": [
        "svc_grid.score(important_test_scaled, mapped_labels_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.696969696969697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RVdzTfS5-fh",
        "colab_type": "text"
      },
      "source": [
        "- Using top 100 PCs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HPCbW3j511p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a3a6a2e-8867-49b6-eb1c-2f3dee0a3996"
      },
      "source": [
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C': np.logspace(-3, 3, 7), 'gamma': ('auto', 'scale')}\n",
        "svc_grid = GridSearchCV(SVC(), parameters, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "svc_grid.fit(pca_train_genes, mapped_labels_train)\n",
        "svc_grid.best_score_"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.689080459770115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1WcB_Ek6PJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bf65da1-c6c6-44c6-a141-da483f177aaa"
      },
      "source": [
        "svc_grid.score(pca_test_genes, mapped_labels_test)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.696969696969697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0JI19Lf9dgu",
        "colab_type": "text"
      },
      "source": [
        "## Considering the inherent order present in the classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV7J1qab9dgv",
        "colab_type": "text"
      },
      "source": [
        "## Ordinal Logistic Regression\n",
        "- AKA ordinal regression\n",
        "- Predict a value which is ordered and also discrete \n",
        "- Seen as a generalization to binary logistic regresion\n",
        "- Relies on correlated genes being removed\n",
        "- Multinomial regression doesnt preserve the inherent ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLRBCnYACfog",
        "colab_type": "text"
      },
      "source": [
        "- Using 3000 most important genes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x6nsHOY9dgv",
        "colab_type": "code",
        "outputId": "26aaefda-b4b9-4a3d-e971-c33145257710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid.fit(np.array(important_train_scaled), np.array(mapped_labels_train))\n",
        "ridge_grid.best_score_"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6920689655172414"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wNcCCvm9dg1",
        "colab_type": "code",
        "outputId": "a38194c2-ee37-4be9-ffe3-eaac2f863b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ridge_grid.score(np.array(important_test_scaled), np.array(mapped_labels_test))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272727272727273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9mAGcQaCnap",
        "colab_type": "text"
      },
      "source": [
        "- Using top 100 PCs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs6G_RDcCrBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a45e94d6-311c-47f4-b053-ceec9e01283e"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid_pc = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid_pc.fit(np.array(pca_train_genes), np.array(mapped_labels_train))\n",
        "ridge_grid_pc.best_score_"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7126436781609196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ie-7_yCyk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1e46933-3e2e-4353-83f4-0f1fce690360"
      },
      "source": [
        "ridge_grid_pc.score(np.array(pca_test_genes), np.array(mapped_labels_test))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7878787878787878"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFgYrqZF9dg4",
        "colab_type": "text"
      },
      "source": [
        "# Small Neural Network\n",
        "- Combine the two priors together: ordinal classes and overlapping\n",
        "- http://orca.st.usm.edu/~zwang/files/rank.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fzxBqR29dg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1QCW4bl9dg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_labels(inputs):\n",
        "    converted = np.zeros((len(inputs), 3))\n",
        "    for i in range(len(inputs)):\n",
        "        current_val = inputs[i]\n",
        "        if current_val == 1:\n",
        "            converted[i][1] = 1\n",
        "            converted[i][0] = 1\n",
        "        elif current_val == 2:\n",
        "            converted[i][0] = 1\n",
        "            converted[i][1] = 1\n",
        "            converted[i][2] = 1\n",
        "        else:\n",
        "          converted[i][0] = 1\n",
        "    return converted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Wofo9e9dg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert target variables \n",
        "converted_train = convert_labels(mapped_labels_train)\n",
        "converted_test = convert_labels(mapped_labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gtjkXVy9dhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to tensors\n",
        "# First only use the PCs\n",
        "train_tensor = torch.tensor(pca_train_genes.astype(np.float32)) \n",
        "test_tensor = torch.tensor(pca_test_genes.astype(np.float32))\n",
        "train_target_tensor = torch.tensor(converted_train)\n",
        "test_target_tensor = torch.tensor(converted_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTM6a-4Q9dhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "# Define datasets and dataloaders\n",
        "testing_dataset = TensorDataset(test_tensor, test_target_tensor)\n",
        "training_dataset = TensorDataset(train_tensor, train_target_tensor)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size = 1)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size = batch_size )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnEzBg0PDCcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EntropyLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, model_output, target):\n",
        "        bc1 = torch.nn.functional.binary_cross_entropy(model_output[:,0], target[:,0])\n",
        "        bc2 = torch.nn.functional.binary_cross_entropy(model_output[:,1], target[:,1])\n",
        "        bc3 = torch.nn.functional.binary_cross_entropy(model_output[:,2], target[:,2])\n",
        "        return bc1 + bc2 + bc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGr75Qh9dhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "# loss_function = torch.nn.NLLLoss()\n",
        "loss_function = EntropyLoss()\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ctxevZ9dhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class rnn_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(rnn_model, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(100, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 3),\n",
        "            # Sigmoid funtion on each of the output nodes\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # The last layer hidden states from BERT are used as embeddings/semantic \n",
        "    # representation for the input and therefore, passed into the MLP\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxqW4rGn9dhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile and train model\n",
        "model = rnn_model()\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok-vVN5D9dhN",
        "colab_type": "code",
        "outputId": "0eda2d2a-227e-4e54-e61e-50ffaaa1ff71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 500\n",
        "for e in range(0, epochs):\n",
        "    model.train()\n",
        "    current_loss = 0\n",
        "    for i, (X, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # To GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device).double()\n",
        "        output = model(X).double()\n",
        "        loss = loss_function(output, y)\n",
        "        loss = loss.type(torch.cuda.FloatTensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'Epoch: {e+1}, Loss: {current_loss/len(train_dataloader)}')"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.3348249357781912\n",
            "Epoch: 2, Loss: 0.33410287177876424\n",
            "Epoch: 3, Loss: 0.33337399834080744\n",
            "Epoch: 4, Loss: 0.33265055598397003\n",
            "Epoch: 5, Loss: 0.3319225354414237\n",
            "Epoch: 6, Loss: 0.33119795824352066\n",
            "Epoch: 7, Loss: 0.3304777623791444\n",
            "Epoch: 8, Loss: 0.3297580884475457\n",
            "Epoch: 9, Loss: 0.32903734202447693\n",
            "Epoch: 10, Loss: 0.3283237042395692\n",
            "Epoch: 11, Loss: 0.32761142442100927\n",
            "Epoch: 12, Loss: 0.32689256809259715\n",
            "Epoch: 13, Loss: 0.32618351947320134\n",
            "Epoch: 14, Loss: 0.3254771973741682\n",
            "Epoch: 15, Loss: 0.32476759231404256\n",
            "Epoch: 16, Loss: 0.3240594675666408\n",
            "Epoch: 17, Loss: 0.3233538556255792\n",
            "Epoch: 18, Loss: 0.32264767744039236\n",
            "Epoch: 19, Loss: 0.32194631546735764\n",
            "Epoch: 20, Loss: 0.32123897852082\n",
            "Epoch: 21, Loss: 0.3205375510611032\n",
            "Epoch: 22, Loss: 0.31983751097792074\n",
            "Epoch: 23, Loss: 0.31913507337632935\n",
            "Epoch: 24, Loss: 0.31843497878626775\n",
            "Epoch: 25, Loss: 0.31773724367744044\n",
            "Epoch: 26, Loss: 0.31703468020025055\n",
            "Epoch: 27, Loss: 0.3163428039927232\n",
            "Epoch: 28, Loss: 0.31564952864458684\n",
            "Epoch: 29, Loss: 0.3149569469847177\n",
            "Epoch: 30, Loss: 0.31427004698075744\n",
            "Epoch: 31, Loss: 0.31358209683706884\n",
            "Epoch: 32, Loss: 0.3129003930248712\n",
            "Epoch: 33, Loss: 0.31221441414795426\n",
            "Epoch: 34, Loss: 0.31153524706238195\n",
            "Epoch: 35, Loss: 0.3108536789291783\n",
            "Epoch: 36, Loss: 0.3101701144325106\n",
            "Epoch: 37, Loss: 0.3094926076500039\n",
            "Epoch: 38, Loss: 0.30880972743034363\n",
            "Epoch: 39, Loss: 0.30813288453378174\n",
            "Epoch: 40, Loss: 0.3074546797495139\n",
            "Epoch: 41, Loss: 0.30678296442094605\n",
            "Epoch: 42, Loss: 0.3061095444779647\n",
            "Epoch: 43, Loss: 0.30543782052240875\n",
            "Epoch: 44, Loss: 0.30476446920319605\n",
            "Epoch: 45, Loss: 0.30409653720102814\n",
            "Epoch: 46, Loss: 0.30343191560946015\n",
            "Epoch: 47, Loss: 0.30276551450553696\n",
            "Epoch: 48, Loss: 0.30210315632192714\n",
            "Epoch: 49, Loss: 0.3014408993093591\n",
            "Epoch: 50, Loss: 0.30078023123113734\n",
            "Epoch: 51, Loss: 0.300115776689429\n",
            "Epoch: 52, Loss: 0.2994595958214057\n",
            "Epoch: 53, Loss: 0.2988049599685167\n",
            "Epoch: 54, Loss: 0.29814988100215006\n",
            "Epoch: 55, Loss: 0.2974928499836671\n",
            "Epoch: 56, Loss: 0.29684339267642873\n",
            "Epoch: 57, Loss: 0.29617908361711\n",
            "Epoch: 58, Loss: 0.2955175226456241\n",
            "Epoch: 59, Loss: 0.29486280013071864\n",
            "Epoch: 60, Loss: 0.2942014607159715\n",
            "Epoch: 61, Loss: 0.2935453947437437\n",
            "Epoch: 62, Loss: 0.29288789590722636\n",
            "Epoch: 63, Loss: 0.29223647439166117\n",
            "Epoch: 64, Loss: 0.2915787920355797\n",
            "Epoch: 65, Loss: 0.29092630861621155\n",
            "Epoch: 66, Loss: 0.29027208489807027\n",
            "Epoch: 67, Loss: 0.28962365773163345\n",
            "Epoch: 68, Loss: 0.28897565564042643\n",
            "Epoch: 69, Loss: 0.28832992911338806\n",
            "Epoch: 70, Loss: 0.2876836732029915\n",
            "Epoch: 71, Loss: 0.28703787962072774\n",
            "Epoch: 72, Loss: 0.2863971418456027\n",
            "Epoch: 73, Loss: 0.2857558335128583\n",
            "Epoch: 74, Loss: 0.2851141750028259\n",
            "Epoch: 75, Loss: 0.28447214161094864\n",
            "Epoch: 76, Loss: 0.283833820176752\n",
            "Epoch: 77, Loss: 0.2831962787007031\n",
            "Epoch: 78, Loss: 0.28255853409829895\n",
            "Epoch: 79, Loss: 0.2819240003039962\n",
            "Epoch: 80, Loss: 0.2812894621961995\n",
            "Epoch: 81, Loss: 0.28066221271690567\n",
            "Epoch: 82, Loss: 0.28002399166947917\n",
            "Epoch: 83, Loss: 0.27939431761440475\n",
            "Epoch: 84, Loss: 0.2787592579659663\n",
            "Epoch: 85, Loss: 0.2781311152012725\n",
            "Epoch: 86, Loss: 0.27750539309100103\n",
            "Epoch: 87, Loss: 0.2768733403400371\n",
            "Epoch: 88, Loss: 0.27624424978306417\n",
            "Epoch: 89, Loss: 0.2756222455125106\n",
            "Epoch: 90, Loss: 0.27499779549084213\n",
            "Epoch: 91, Loss: 0.27437201456019755\n",
            "Epoch: 92, Loss: 0.2737503792894514\n",
            "Epoch: 93, Loss: 0.2731272731172411\n",
            "Epoch: 94, Loss: 0.27250654211169795\n",
            "Epoch: 95, Loss: 0.271887207501813\n",
            "Epoch: 96, Loss: 0.2712699018026653\n",
            "Epoch: 97, Loss: 0.2706535140934743\n",
            "Epoch: 98, Loss: 0.27003844160782664\n",
            "Epoch: 99, Loss: 0.269426309356564\n",
            "Epoch: 100, Loss: 0.26881073336852224\n",
            "Epoch: 101, Loss: 0.2681986407229775\n",
            "Epoch: 102, Loss: 0.26758840327200134\n",
            "Epoch: 103, Loss: 0.2669748177653865\n",
            "Epoch: 104, Loss: 0.2663691110516849\n",
            "Epoch: 105, Loss: 0.26576185893071325\n",
            "Epoch: 106, Loss: 0.265150019212773\n",
            "Epoch: 107, Loss: 0.26454038682736847\n",
            "Epoch: 108, Loss: 0.2639321637781043\n",
            "Epoch: 109, Loss: 0.2633212977334073\n",
            "Epoch: 110, Loss: 0.2627164372488072\n",
            "Epoch: 111, Loss: 0.26210907454553406\n",
            "Epoch: 112, Loss: 0.26150846167614583\n",
            "Epoch: 113, Loss: 0.26090397215203237\n",
            "Epoch: 114, Loss: 0.2603030369469994\n",
            "Epoch: 115, Loss: 0.25970518196883957\n",
            "Epoch: 116, Loss: 0.25910816498492895\n",
            "Epoch: 117, Loss: 0.25851045392061534\n",
            "Epoch: 118, Loss: 0.2579142796365838\n",
            "Epoch: 119, Loss: 0.2573230325391418\n",
            "Epoch: 120, Loss: 0.2567295741878058\n",
            "Epoch: 121, Loss: 0.25613654051956375\n",
            "Epoch: 122, Loss: 0.25554937634028885\n",
            "Epoch: 123, Loss: 0.2549617004237677\n",
            "Epoch: 124, Loss: 0.25437243165154205\n",
            "Epoch: 125, Loss: 0.2537872905009671\n",
            "Epoch: 126, Loss: 0.25320531545501007\n",
            "Epoch: 127, Loss: 0.25262295886089925\n",
            "Epoch: 128, Loss: 0.25204063912755564\n",
            "Epoch: 129, Loss: 0.2514656037092209\n",
            "Epoch: 130, Loss: 0.25088558189178767\n",
            "Epoch: 131, Loss: 0.25030694117671565\n",
            "Epoch: 132, Loss: 0.24973469502047488\n",
            "Epoch: 133, Loss: 0.24915336699862228\n",
            "Epoch: 134, Loss: 0.24858276546001434\n",
            "Epoch: 135, Loss: 0.24800586935720945\n",
            "Epoch: 136, Loss: 0.2474381260966\n",
            "Epoch: 137, Loss: 0.24686186133246674\n",
            "Epoch: 138, Loss: 0.24629502704269007\n",
            "Epoch: 139, Loss: 0.2457266681288418\n",
            "Epoch: 140, Loss: 0.24516208626722036\n",
            "Epoch: 141, Loss: 0.2445967934633556\n",
            "Epoch: 142, Loss: 0.2440298395721536\n",
            "Epoch: 143, Loss: 0.24347709080106333\n",
            "Epoch: 144, Loss: 0.24291151997290159\n",
            "Epoch: 145, Loss: 0.24235619839869046\n",
            "Epoch: 146, Loss: 0.2418037625519853\n",
            "Epoch: 147, Loss: 0.24124289382445185\n",
            "Epoch: 148, Loss: 0.24069292921768992\n",
            "Epoch: 149, Loss: 0.240134480752443\n",
            "Epoch: 150, Loss: 0.23958376089209005\n",
            "Epoch: 151, Loss: 0.2390295969028222\n",
            "Epoch: 152, Loss: 0.23847436159849167\n",
            "Epoch: 153, Loss: 0.23792296177462527\n",
            "Epoch: 154, Loss: 0.2373745374773678\n",
            "Epoch: 155, Loss: 0.2368237666393581\n",
            "Epoch: 156, Loss: 0.23627503765256783\n",
            "Epoch: 157, Loss: 0.23572905989069687\n",
            "Epoch: 158, Loss: 0.2351864619474662\n",
            "Epoch: 159, Loss: 0.23464228644182808\n",
            "Epoch: 160, Loss: 0.23410016063012576\n",
            "Epoch: 161, Loss: 0.23356016803728907\n",
            "Epoch: 162, Loss: 0.23301743363079272\n",
            "Epoch: 163, Loss: 0.2324717601663188\n",
            "Epoch: 164, Loss: 0.23193085389701942\n",
            "Epoch: 165, Loss: 0.2313888578822738\n",
            "Epoch: 166, Loss: 0.2308433495069805\n",
            "Epoch: 167, Loss: 0.23030618068419004\n",
            "Epoch: 168, Loss: 0.22976334040102206\n",
            "Epoch: 169, Loss: 0.229231175623442\n",
            "Epoch: 170, Loss: 0.228693110378165\n",
            "Epoch: 171, Loss: 0.22815932960886703\n",
            "Epoch: 172, Loss: 0.22762352149737508\n",
            "Epoch: 173, Loss: 0.22708802591813237\n",
            "Epoch: 174, Loss: 0.2265571520516747\n",
            "Epoch: 175, Loss: 0.22602861256975876\n",
            "Epoch: 176, Loss: 0.22549024106640564\n",
            "Epoch: 177, Loss: 0.22496102044456884\n",
            "Epoch: 178, Loss: 0.22442748319161565\n",
            "Epoch: 179, Loss: 0.22389817982912064\n",
            "Epoch: 180, Loss: 0.2233734072039002\n",
            "Epoch: 181, Loss: 0.22284033659257388\n",
            "Epoch: 182, Loss: 0.22231428089894748\n",
            "Epoch: 183, Loss: 0.22178469245371066\n",
            "Epoch: 184, Loss: 0.22126368943013644\n",
            "Epoch: 185, Loss: 0.22073448959149813\n",
            "Epoch: 186, Loss: 0.22021631425932833\n",
            "Epoch: 187, Loss: 0.2196903232681124\n",
            "Epoch: 188, Loss: 0.21916912103954114\n",
            "Epoch: 189, Loss: 0.2186499908566475\n",
            "Epoch: 190, Loss: 0.21813101164604487\n",
            "Epoch: 191, Loss: 0.21760865183253036\n",
            "Epoch: 192, Loss: 0.21708942988985463\n",
            "Epoch: 193, Loss: 0.21656787199409386\n",
            "Epoch: 194, Loss: 0.21605276983035238\n",
            "Epoch: 195, Loss: 0.21553950521506762\n",
            "Epoch: 196, Loss: 0.21502203964873365\n",
            "Epoch: 197, Loss: 0.21450258909087433\n",
            "Epoch: 198, Loss: 0.2139850003938926\n",
            "Epoch: 199, Loss: 0.2134714479509153\n",
            "Epoch: 200, Loss: 0.21295127311819478\n",
            "Epoch: 201, Loss: 0.2124407169850249\n",
            "Epoch: 202, Loss: 0.21192723120513715\n",
            "Epoch: 203, Loss: 0.21141866869048068\n",
            "Epoch: 204, Loss: 0.21090742984884664\n",
            "Epoch: 205, Loss: 0.21039595729426333\n",
            "Epoch: 206, Loss: 0.2098886472614188\n",
            "Epoch: 207, Loss: 0.20938136310953842\n",
            "Epoch: 208, Loss: 0.2088752020346491\n",
            "Epoch: 209, Loss: 0.2083700330633866\n",
            "Epoch: 210, Loss: 0.2078655240567107\n",
            "Epoch: 211, Loss: 0.20736525325398697\n",
            "Epoch: 212, Loss: 0.20685898394961105\n",
            "Epoch: 213, Loss: 0.20635999150966344\n",
            "Epoch: 214, Loss: 0.20585384102244125\n",
            "Epoch: 215, Loss: 0.20535800762866674\n",
            "Epoch: 216, Loss: 0.20485592162922808\n",
            "Epoch: 217, Loss: 0.20435895045336924\n",
            "Epoch: 218, Loss: 0.20386540477997378\n",
            "Epoch: 219, Loss: 0.203367402286906\n",
            "Epoch: 220, Loss: 0.20287503988335007\n",
            "Epoch: 221, Loss: 0.2023776046147472\n",
            "Epoch: 222, Loss: 0.20188592452751963\n",
            "Epoch: 223, Loss: 0.20140033783881287\n",
            "Epoch: 224, Loss: 0.20090846325221814\n",
            "Epoch: 225, Loss: 0.20041728333423012\n",
            "Epoch: 226, Loss: 0.19992933500754206\n",
            "Epoch: 227, Loss: 0.19943893210668312\n",
            "Epoch: 228, Loss: 0.1989535299178801\n",
            "Epoch: 229, Loss: 0.1984723120143539\n",
            "Epoch: 230, Loss: 0.19798399547213003\n",
            "Epoch: 231, Loss: 0.19750138705498294\n",
            "Epoch: 232, Loss: 0.19701425064551203\n",
            "Epoch: 233, Loss: 0.19653623296242012\n",
            "Epoch: 234, Loss: 0.1960513376091656\n",
            "Epoch: 235, Loss: 0.19557182726107145\n",
            "Epoch: 236, Loss: 0.19508806812135795\n",
            "Epoch: 237, Loss: 0.19461499076140554\n",
            "Epoch: 238, Loss: 0.19413299466434278\n",
            "Epoch: 239, Loss: 0.19365651768289113\n",
            "Epoch: 240, Loss: 0.19318233116676933\n",
            "Epoch: 241, Loss: 0.19270840895019078\n",
            "Epoch: 242, Loss: 0.19223711031832194\n",
            "Epoch: 243, Loss: 0.19176923758105227\n",
            "Epoch: 244, Loss: 0.19129998727064385\n",
            "Epoch: 245, Loss: 0.19082845159267126\n",
            "Epoch: 246, Loss: 0.19036541330186943\n",
            "Epoch: 247, Loss: 0.18989945575594902\n",
            "Epoch: 248, Loss: 0.18943409935424202\n",
            "Epoch: 249, Loss: 0.18896914430354772\n",
            "Epoch: 250, Loss: 0.1885072735971526\n",
            "Epoch: 251, Loss: 0.18804211698864637\n",
            "Epoch: 252, Loss: 0.1875833106276236\n",
            "Epoch: 253, Loss: 0.18711950296634122\n",
            "Epoch: 254, Loss: 0.18666011996959386\n",
            "Epoch: 255, Loss: 0.18620087931814946\n",
            "Epoch: 256, Loss: 0.1857424011748088\n",
            "Epoch: 257, Loss: 0.18528276466225324\n",
            "Epoch: 258, Loss: 0.18482917449191996\n",
            "Epoch: 259, Loss: 0.18436837490451963\n",
            "Epoch: 260, Loss: 0.18391288130691177\n",
            "Epoch: 261, Loss: 0.18345498724987633\n",
            "Epoch: 262, Loss: 0.1829977243354446\n",
            "Epoch: 263, Loss: 0.1825410547225099\n",
            "Epoch: 264, Loss: 0.18208743631839752\n",
            "Epoch: 265, Loss: 0.18163252386607623\n",
            "Epoch: 266, Loss: 0.18117692654854373\n",
            "Epoch: 267, Loss: 0.18072128433145976\n",
            "Epoch: 268, Loss: 0.1802726517382421\n",
            "Epoch: 269, Loss: 0.1798214730071394\n",
            "Epoch: 270, Loss: 0.17936891964391657\n",
            "Epoch: 271, Loss: 0.1789201023547273\n",
            "Epoch: 272, Loss: 0.17847244421902456\n",
            "Epoch: 273, Loss: 0.1780237006513696\n",
            "Epoch: 274, Loss: 0.17757449612805717\n",
            "Epoch: 275, Loss: 0.17712879925966263\n",
            "Epoch: 276, Loss: 0.17668809859376206\n",
            "Epoch: 277, Loss: 0.17623963167792872\n",
            "Epoch: 278, Loss: 0.17579783832556323\n",
            "Epoch: 279, Loss: 0.17535583949402758\n",
            "Epoch: 280, Loss: 0.17491331716117106\n",
            "Epoch: 281, Loss: 0.17447393603230776\n",
            "Epoch: 282, Loss: 0.17403222502846466\n",
            "Epoch: 283, Loss: 0.17359446832223943\n",
            "Epoch: 284, Loss: 0.1731521877411165\n",
            "Epoch: 285, Loss: 0.17271681776956507\n",
            "Epoch: 286, Loss: 0.17228100468453608\n",
            "Epoch: 287, Loss: 0.17184008403044\n",
            "Epoch: 288, Loss: 0.17140100955178864\n",
            "Epoch: 289, Loss: 0.17096007987856865\n",
            "Epoch: 290, Loss: 0.17051925000391507\n",
            "Epoch: 291, Loss: 0.17008055589701\n",
            "Epoch: 292, Loss: 0.1696404085347527\n",
            "Epoch: 293, Loss: 0.1692039684245461\n",
            "Epoch: 294, Loss: 0.1687643975019455\n",
            "Epoch: 295, Loss: 0.16832704116639338\n",
            "Epoch: 296, Loss: 0.1678874647538913\n",
            "Epoch: 297, Loss: 0.16744914925412127\n",
            "Epoch: 298, Loss: 0.16701287619377436\n",
            "Epoch: 299, Loss: 0.16657435835192078\n",
            "Epoch: 300, Loss: 0.1661362669577724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rISIjKfj9dhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = []\n",
        "for i, (X, y) in enumerate(test_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    X = X.to(device)\n",
        "    output = model(X)   \n",
        "    test_outputs.append(output.cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dwl9QN0K17u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs_converted = []\n",
        "for i in range(len(test_outputs)):\n",
        "  current = test_outputs[i][0]\n",
        "  add = [1 if x >= 0.50 else 0 for x in current]\n",
        "  test_outputs_converted.append(add)\n",
        "test_outputs_converted = np.array(test_outputs_converted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORdWugkBG2j_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a8d4afc-8337-4cf7-b0f0-ddb448d0072f"
      },
      "source": [
        "accuracy = 0\n",
        "missed = []\n",
        "for i in range(len(test_outputs_converted)):\n",
        "  if collections.Counter(test_outputs_converted[i]) == collections.Counter(converted_test[i]):\n",
        "    accuracy += 1\n",
        "  else:\n",
        "    missed.append(i)\n",
        "print(f'Testing Accuracy: {accuracy/33}')"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.8181818181818182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2c64Sea9dhU",
        "colab_type": "text"
      },
      "source": [
        "## Feature Analysis on the top performing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywt8j2S89dhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.feature_selection import RFE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr0SoGSiRvI-",
        "colab_type": "text"
      },
      "source": [
        "# Permutation Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGQgknZDH9uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# permutation importance\n",
        "pi = permutation_importance(ridge_grid.best_estimator_, important_train_scaled, mapped_labels_train, n_repeats=10, random_state=0)\n",
        "pi_mean_vals = pi['importances_mean']\n",
        "cols = pd.DataFrame(important_train_scaled).columns\n",
        "pi_importances_pairs = zip(cols, pi_mean_vals)\n",
        "pi_pairs_sorted = sorted(pi_importances_pairs, key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ROkLoJKKsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all the non-zero importance genes\n",
        "non_zero_pi = [x[0] for x in pi_pairs_sorted if x[1] > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns3IEprqR3n3",
        "colab_type": "text"
      },
      "source": [
        "# Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS168TyrLb3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rfe\n",
        "selector = RFE(ridge_grid.best_estimator_, 1, step=1)\n",
        "selector = selector.fit(important_train_scaled, mapped_labels_train)\n",
        "ranking_pairs = zip(pd.DataFrame(important_train_scaled).columns, selector.ranking_)\n",
        "ranking_pairs_sorted = sorted(ranking_pairs, key=lambda x: x[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acSD9CpVQ70X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_100_rfe = []\n",
        "for i in range(100):\n",
        "  top_100_rfe.append(ranking_pairs_sorted[i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyLTkpusWdx4",
        "colab_type": "text"
      },
      "source": [
        "# Rerunning the top models using identified features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue0od7EEWqwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "important_genes_pi_train = pd.DataFrame(important_train_scaled)[non_zero_pi]\n",
        "important_genes_pi_test = pd.DataFrame(important_test_scaled)[non_zero_pi]\n",
        "important_genes_rfe_train = pd.DataFrame(important_train_scaled)[top_100_rfe]\n",
        "important_genes_rfe_test = pd.DataFrame(important_test_scaled)[top_100_rfe]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGLWe99kWh41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b09f6269-9b1a-42fc-96ec-528aeea6316f"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid.fit(np.array(important_genes_pi_train), np.array(mapped_labels_train))\n",
        "ridge_grid.best_score_"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7503448275862069"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ssQmS3wXyex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d5db9ad-c360-474d-9ea0-820846965670"
      },
      "source": [
        "ridge_grid.score(np.array(important_genes_pi_test), np.array(mapped_labels_test))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272727272727273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbQPXFhMYbav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4471afd-e1ec-46ba-d169-8a28777f8017"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid.fit(np.array(important_genes_rfe_train), np.array(mapped_labels_train))\n",
        "ridge_grid.best_score_"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8597701149425288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezsT1MRYl6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b93174fa-71e4-4647-9fbc-8e3ab6946e03"
      },
      "source": [
        "ridge_grid.score(np.array(important_genes_rfe_test), np.array(mapped_labels_test))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7878787878787878"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rbaa8DoZwky",
        "colab_type": "text"
      },
      "source": [
        "# Retrain Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Ozg17gWr2D",
        "colab_type": "text"
      },
      "source": [
        "# NOTE: This is on the RFE ones, maybe do it on PI as well?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ohHFulmU0Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tensor = torch.tensor(important_genes_rfe_train.values.astype(np.float32)) \n",
        "test_tensor = torch.tensor(important_genes_rfe_test.values.astype(np.float32))\n",
        "batch_size = 16\n",
        "# Define datasets and dataloaders\n",
        "testing_dataset = TensorDataset(test_tensor, test_target_tensor)\n",
        "training_dataset = TensorDataset(train_tensor, train_target_tensor)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size = 1)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size = batch_size )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbgeRI-FaS10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class rnn_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(rnn_model, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(len(important_genes_rfe_train.columns), 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 3),\n",
        "            # Sigmoid funtion on each of the output nodes\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # The last layer hidden states from BERT are used as embeddings/semantic \n",
        "    # representation for the input and therefore, passed into the MLP\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdJBry1aYVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile and train model\n",
        "model = rnn_model()\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqh8FZCaab07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42648bf3-5295-4784-d1d5-1ce3e827ffb7"
      },
      "source": [
        "epochs = 500\n",
        "for e in range(0, epochs):\n",
        "    model.train()\n",
        "    current_loss = 0\n",
        "    for i, (X, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # To GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device).double()\n",
        "        output = model(X).double()\n",
        "        loss = loss_function(output, y)\n",
        "        loss = loss.type(torch.cuda.FloatTensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'Epoch: {e+1}, Loss: {current_loss/len(train_dataloader)}')"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 2.0757108989514803\n",
            "Epoch: 2, Loss: 2.0717079639434814\n",
            "Epoch: 3, Loss: 2.0677985894052604\n",
            "Epoch: 4, Loss: 2.0639085142236007\n",
            "Epoch: 5, Loss: 2.060035366761057\n",
            "Epoch: 6, Loss: 2.056169949079815\n",
            "Epoch: 7, Loss: 2.052313101919074\n",
            "Epoch: 8, Loss: 2.0484586263957776\n",
            "Epoch: 9, Loss: 2.04461101481789\n",
            "Epoch: 10, Loss: 2.040764645526284\n",
            "Epoch: 11, Loss: 2.036909718262522\n",
            "Epoch: 12, Loss: 2.033042882618151\n",
            "Epoch: 13, Loss: 2.029167708597685\n",
            "Epoch: 14, Loss: 2.0252770310954045\n",
            "Epoch: 15, Loss: 2.021361225529721\n",
            "Epoch: 16, Loss: 2.0174418248628316\n",
            "Epoch: 17, Loss: 2.0135178691462468\n",
            "Epoch: 18, Loss: 2.0095832912545455\n",
            "Epoch: 19, Loss: 2.0056339439592863\n",
            "Epoch: 20, Loss: 2.0016687167318246\n",
            "Epoch: 21, Loss: 1.9976945676301654\n",
            "Epoch: 22, Loss: 1.993718605292471\n",
            "Epoch: 23, Loss: 1.9897354778490568\n",
            "Epoch: 24, Loss: 1.985734920752676\n",
            "Epoch: 25, Loss: 1.9817234654175608\n",
            "Epoch: 26, Loss: 1.9777027933221114\n",
            "Epoch: 27, Loss: 1.9736679227728593\n",
            "Epoch: 28, Loss: 1.9696210183595355\n",
            "Epoch: 29, Loss: 1.965563372561806\n",
            "Epoch: 30, Loss: 1.9614910138280768\n",
            "Epoch: 31, Loss: 1.9573966892142045\n",
            "Epoch: 32, Loss: 1.9532860642985295\n",
            "Epoch: 33, Loss: 1.9491609460429142\n",
            "Epoch: 34, Loss: 1.9450194082762067\n",
            "Epoch: 35, Loss: 1.9408525479467291\n",
            "Epoch: 36, Loss: 1.9366624982733476\n",
            "Epoch: 37, Loss: 1.9324520889081453\n",
            "Epoch: 38, Loss: 1.9282199269846867\n",
            "Epoch: 39, Loss: 1.9239654101823505\n",
            "Epoch: 40, Loss: 1.9196885949686955\n",
            "Epoch: 41, Loss: 1.9153953790664673\n",
            "Epoch: 42, Loss: 1.9110824434380782\n",
            "Epoch: 43, Loss: 1.9067539980537014\n",
            "Epoch: 44, Loss: 1.90240142219945\n",
            "Epoch: 45, Loss: 1.8980287940878617\n",
            "Epoch: 46, Loss: 1.8936325374402498\n",
            "Epoch: 47, Loss: 1.8891959629560773\n",
            "Epoch: 48, Loss: 1.8847241025221975\n",
            "Epoch: 49, Loss: 1.8802076013464677\n",
            "Epoch: 50, Loss: 1.875656805540386\n",
            "Epoch: 51, Loss: 1.8710764019112838\n",
            "Epoch: 52, Loss: 1.8664634102269222\n",
            "Epoch: 53, Loss: 1.8618046609978927\n",
            "Epoch: 54, Loss: 1.8571179415050305\n",
            "Epoch: 55, Loss: 1.8524122489126105\n",
            "Epoch: 56, Loss: 1.847681384337576\n",
            "Epoch: 57, Loss: 1.842913828398052\n",
            "Epoch: 58, Loss: 1.8381137785158659\n",
            "Epoch: 59, Loss: 1.8332883056841398\n",
            "Epoch: 60, Loss: 1.8284351951197575\n",
            "Epoch: 61, Loss: 1.823546196285047\n",
            "Epoch: 62, Loss: 1.8186222314834595\n",
            "Epoch: 63, Loss: 1.8136636332461709\n",
            "Epoch: 64, Loss: 1.8086653885088468\n",
            "Epoch: 65, Loss: 1.8036396879898875\n",
            "Epoch: 66, Loss: 1.7985875041861283\n",
            "Epoch: 67, Loss: 1.7935020986356234\n",
            "Epoch: 68, Loss: 1.7883843936418231\n",
            "Epoch: 69, Loss: 1.783234483317325\n",
            "Epoch: 70, Loss: 1.778050190524051\n",
            "Epoch: 71, Loss: 1.772822612210324\n",
            "Epoch: 72, Loss: 1.767551629166854\n",
            "Epoch: 73, Loss: 1.7622606503336053\n",
            "Epoch: 74, Loss: 1.7569423223796643\n",
            "Epoch: 75, Loss: 1.7515862929193597\n",
            "Epoch: 76, Loss: 1.7461902279602854\n",
            "Epoch: 77, Loss: 1.7407631560375816\n",
            "Epoch: 78, Loss: 1.7353148648613377\n",
            "Epoch: 79, Loss: 1.7298410315262644\n",
            "Epoch: 80, Loss: 1.7243364672911794\n",
            "Epoch: 81, Loss: 1.7187969119925248\n",
            "Epoch: 82, Loss: 1.7132233632238287\n",
            "Epoch: 83, Loss: 1.7076129097687571\n",
            "Epoch: 84, Loss: 1.7019747495651245\n",
            "Epoch: 85, Loss: 1.6963068435066624\n",
            "Epoch: 86, Loss: 1.6906064058604993\n",
            "Epoch: 87, Loss: 1.6848701677824323\n",
            "Epoch: 88, Loss: 1.679091309246264\n",
            "Epoch: 89, Loss: 1.6732808100549799\n",
            "Epoch: 90, Loss: 1.66744579766926\n",
            "Epoch: 91, Loss: 1.6615839631933915\n",
            "Epoch: 92, Loss: 1.6556952313372963\n",
            "Epoch: 93, Loss: 1.6497720793673867\n",
            "Epoch: 94, Loss: 1.64381238033897\n",
            "Epoch: 95, Loss: 1.6378205387215865\n",
            "Epoch: 96, Loss: 1.6318066935790212\n",
            "Epoch: 97, Loss: 1.6257654804932444\n",
            "Epoch: 98, Loss: 1.6196930659444708\n",
            "Epoch: 99, Loss: 1.613596796989441\n",
            "Epoch: 100, Loss: 1.6074638994116532\n",
            "Epoch: 101, Loss: 1.6012881617797048\n",
            "Epoch: 102, Loss: 1.5950693707717092\n",
            "Epoch: 103, Loss: 1.588816893728156\n",
            "Epoch: 104, Loss: 1.5825402422955162\n",
            "Epoch: 105, Loss: 1.5762422649483931\n",
            "Epoch: 106, Loss: 1.5699205649526495\n",
            "Epoch: 107, Loss: 1.5635761649985063\n",
            "Epoch: 108, Loss: 1.5572103324689364\n",
            "Epoch: 109, Loss: 1.5508161845960116\n",
            "Epoch: 110, Loss: 1.544396306339063\n",
            "Epoch: 111, Loss: 1.5379529250295538\n",
            "Epoch: 112, Loss: 1.5314828659358777\n",
            "Epoch: 113, Loss: 1.5249902198189182\n",
            "Epoch: 114, Loss: 1.5184874283640009\n",
            "Epoch: 115, Loss: 1.5119728602861102\n",
            "Epoch: 116, Loss: 1.5054507004587274\n",
            "Epoch: 117, Loss: 1.4989113995903416\n",
            "Epoch: 118, Loss: 1.4923588539424695\n",
            "Epoch: 119, Loss: 1.485800078040675\n",
            "Epoch: 120, Loss: 1.4792307489796688\n",
            "Epoch: 121, Loss: 1.4726470520621853\n",
            "Epoch: 122, Loss: 1.4660512208938599\n",
            "Epoch: 123, Loss: 1.459448544602645\n",
            "Epoch: 124, Loss: 1.4528475686123496\n",
            "Epoch: 125, Loss: 1.4462477470699109\n",
            "Epoch: 126, Loss: 1.4396445751190186\n",
            "Epoch: 127, Loss: 1.4330385233226575\n",
            "Epoch: 128, Loss: 1.4264291211178428\n",
            "Epoch: 129, Loss: 1.4198153395401805\n",
            "Epoch: 130, Loss: 1.4132061255605597\n",
            "Epoch: 131, Loss: 1.4065999357323897\n",
            "Epoch: 132, Loss: 1.3999982256638377\n",
            "Epoch: 133, Loss: 1.3933881960417096\n",
            "Epoch: 134, Loss: 1.3867759139914262\n",
            "Epoch: 135, Loss: 1.3801651691135608\n",
            "Epoch: 136, Loss: 1.373566000085128\n",
            "Epoch: 137, Loss: 1.366978739437304\n",
            "Epoch: 138, Loss: 1.3603933171222085\n",
            "Epoch: 139, Loss: 1.3538070540679128\n",
            "Epoch: 140, Loss: 1.3472257036911814\n",
            "Epoch: 141, Loss: 1.3406462167438709\n",
            "Epoch: 142, Loss: 1.3340705821388645\n",
            "Epoch: 143, Loss: 1.3275080166364972\n",
            "Epoch: 144, Loss: 1.3209668146936517\n",
            "Epoch: 145, Loss: 1.3144461355711285\n",
            "Epoch: 146, Loss: 1.3079383185035305\n",
            "Epoch: 147, Loss: 1.3014389151021053\n",
            "Epoch: 148, Loss: 1.2949497636995817\n",
            "Epoch: 149, Loss: 1.2884782301752191\n",
            "Epoch: 150, Loss: 1.2820199916237278\n",
            "Epoch: 151, Loss: 1.2755792893861468\n",
            "Epoch: 152, Loss: 1.2691560732690912\n",
            "Epoch: 153, Loss: 1.2627498664354022\n",
            "Epoch: 154, Loss: 1.2563673508794684\n",
            "Epoch: 155, Loss: 1.2500144807915938\n",
            "Epoch: 156, Loss: 1.2436906475769847\n",
            "Epoch: 157, Loss: 1.2373912334442139\n",
            "Epoch: 158, Loss: 1.2311142745770907\n",
            "Epoch: 159, Loss: 1.2248644891538119\n",
            "Epoch: 160, Loss: 1.218642837122867\n",
            "Epoch: 161, Loss: 1.212446231591074\n",
            "Epoch: 162, Loss: 1.2062736279086064\n",
            "Epoch: 163, Loss: 1.2001255280093144\n",
            "Epoch: 164, Loss: 1.1940039866848995\n",
            "Epoch: 165, Loss: 1.1879099921176308\n",
            "Epoch: 166, Loss: 1.18184978397269\n",
            "Epoch: 167, Loss: 1.1758194503031278\n",
            "Epoch: 168, Loss: 1.1698138368757147\n",
            "Epoch: 169, Loss: 1.1638419157580326\n",
            "Epoch: 170, Loss: 1.157906030353747\n",
            "Epoch: 171, Loss: 1.1520049854328758\n",
            "Epoch: 172, Loss: 1.1461362399552997\n",
            "Epoch: 173, Loss: 1.1402958913853294\n",
            "Epoch: 174, Loss: 1.134486314497496\n",
            "Epoch: 175, Loss: 1.1287079704435248\n",
            "Epoch: 176, Loss: 1.1229576374355115\n",
            "Epoch: 177, Loss: 1.1172381294401068\n",
            "Epoch: 178, Loss: 1.111552800002851\n",
            "Epoch: 179, Loss: 1.1059010593514693\n",
            "Epoch: 180, Loss: 1.1002858249764693\n",
            "Epoch: 181, Loss: 1.0947095406682867\n",
            "Epoch: 182, Loss: 1.089171334316856\n",
            "Epoch: 183, Loss: 1.0836701142160516\n",
            "Epoch: 184, Loss: 1.0782021346845125\n",
            "Epoch: 185, Loss: 1.0727687509436357\n",
            "Epoch: 186, Loss: 1.067366571802842\n",
            "Epoch: 187, Loss: 1.0620011812762211\n",
            "Epoch: 188, Loss: 1.0566769493253607\n",
            "Epoch: 189, Loss: 1.0513869900452464\n",
            "Epoch: 190, Loss: 1.0461324767062539\n",
            "Epoch: 191, Loss: 1.0409185133482282\n",
            "Epoch: 192, Loss: 1.0357415236924823\n",
            "Epoch: 193, Loss: 1.0305991455128318\n",
            "Epoch: 194, Loss: 1.0254931261664944\n",
            "Epoch: 195, Loss: 1.0204269007632607\n",
            "Epoch: 196, Loss: 1.0153992677989758\n",
            "Epoch: 197, Loss: 1.0104119275745593\n",
            "Epoch: 198, Loss: 1.0054653035966974\n",
            "Epoch: 199, Loss: 1.0005601832741184\n",
            "Epoch: 200, Loss: 0.9956977869334974\n",
            "Epoch: 201, Loss: 0.99087844710601\n",
            "Epoch: 202, Loss: 0.9860987694639909\n",
            "Epoch: 203, Loss: 0.9813566709819593\n",
            "Epoch: 204, Loss: 0.9766517093307093\n",
            "Epoch: 205, Loss: 0.9719856506899783\n",
            "Epoch: 206, Loss: 0.9673598314586439\n",
            "Epoch: 207, Loss: 0.962775308834879\n",
            "Epoch: 208, Loss: 0.958232769840642\n",
            "Epoch: 209, Loss: 0.9537318788076702\n",
            "Epoch: 210, Loss: 0.949271885972274\n",
            "Epoch: 211, Loss: 0.9448521701913131\n",
            "Epoch: 212, Loss: 0.9404702437551398\n",
            "Epoch: 213, Loss: 0.9361266650651631\n",
            "Epoch: 214, Loss: 0.9318235579289889\n",
            "Epoch: 215, Loss: 0.9275593757629395\n",
            "Epoch: 216, Loss: 0.9233333405695463\n",
            "Epoch: 217, Loss: 0.9191422274238185\n",
            "Epoch: 218, Loss: 0.9149868174603111\n",
            "Epoch: 219, Loss: 0.9108732907395614\n",
            "Epoch: 220, Loss: 0.906800483402453\n",
            "Epoch: 221, Loss: 0.90276658221295\n",
            "Epoch: 222, Loss: 0.8987690367196736\n",
            "Epoch: 223, Loss: 0.894805930162731\n",
            "Epoch: 224, Loss: 0.890873228248797\n",
            "Epoch: 225, Loss: 0.8869769290873879\n",
            "Epoch: 226, Loss: 0.8831163174227664\n",
            "Epoch: 227, Loss: 0.8792916065768192\n",
            "Epoch: 228, Loss: 0.8755006194114685\n",
            "Epoch: 229, Loss: 0.8717465055616278\n",
            "Epoch: 230, Loss: 0.8680292681643837\n",
            "Epoch: 231, Loss: 0.8643472508380288\n",
            "Epoch: 232, Loss: 0.8607016676350644\n",
            "Epoch: 233, Loss: 0.8570911037294489\n",
            "Epoch: 234, Loss: 0.8535152109045732\n",
            "Epoch: 235, Loss: 0.8499750871407358\n",
            "Epoch: 236, Loss: 0.8464688658714294\n",
            "Epoch: 237, Loss: 0.8429921018449884\n",
            "Epoch: 238, Loss: 0.8395456859939977\n",
            "Epoch: 239, Loss: 0.836132498163926\n",
            "Epoch: 240, Loss: 0.8327524740444986\n",
            "Epoch: 241, Loss: 0.8294032169015784\n",
            "Epoch: 242, Loss: 0.8260866952569861\n",
            "Epoch: 243, Loss: 0.8228008323594144\n",
            "Epoch: 244, Loss: 0.8195471889094302\n",
            "Epoch: 245, Loss: 0.8163230450529801\n",
            "Epoch: 246, Loss: 0.8131301842237774\n",
            "Epoch: 247, Loss: 0.8099675319696727\n",
            "Epoch: 248, Loss: 0.8068356168897528\n",
            "Epoch: 249, Loss: 0.8037355871577012\n",
            "Epoch: 250, Loss: 0.8006674412049746\n",
            "Epoch: 251, Loss: 0.7976297626369878\n",
            "Epoch: 252, Loss: 0.7946234031727439\n",
            "Epoch: 253, Loss: 0.7916478546042192\n",
            "Epoch: 254, Loss: 0.7887015703477358\n",
            "Epoch: 255, Loss: 0.7857829426464281\n",
            "Epoch: 256, Loss: 0.7828918852304158\n",
            "Epoch: 257, Loss: 0.7800280263549403\n",
            "Epoch: 258, Loss: 0.7771932765057212\n",
            "Epoch: 259, Loss: 0.7743848813207526\n",
            "Epoch: 260, Loss: 0.7716046822698492\n",
            "Epoch: 261, Loss: 0.7688512206077576\n",
            "Epoch: 262, Loss: 0.7661224211517134\n",
            "Epoch: 263, Loss: 0.7634174980615315\n",
            "Epoch: 264, Loss: 0.7607363995752836\n",
            "Epoch: 265, Loss: 0.7580808903041639\n",
            "Epoch: 266, Loss: 0.7554517654996169\n",
            "Epoch: 267, Loss: 0.7528472668246219\n",
            "Epoch: 268, Loss: 0.7502676123066953\n",
            "Epoch: 269, Loss: 0.7477117996466788\n",
            "Epoch: 270, Loss: 0.7451800939283872\n",
            "Epoch: 271, Loss: 0.7426716450013613\n",
            "Epoch: 272, Loss: 0.7401848749110573\n",
            "Epoch: 273, Loss: 0.7377221772545263\n",
            "Epoch: 274, Loss: 0.7352811364751113\n",
            "Epoch: 275, Loss: 0.7328619862857618\n",
            "Epoch: 276, Loss: 0.7304645588523463\n",
            "Epoch: 277, Loss: 0.7280892290567097\n",
            "Epoch: 278, Loss: 0.7257359984673952\n",
            "Epoch: 279, Loss: 0.7234037565557581\n",
            "Epoch: 280, Loss: 0.721092186476055\n",
            "Epoch: 281, Loss: 0.7188000412363755\n",
            "Epoch: 282, Loss: 0.7165260769818959\n",
            "Epoch: 283, Loss: 0.7142669495783354\n",
            "Epoch: 284, Loss: 0.7120280501089598\n",
            "Epoch: 285, Loss: 0.7098095589562466\n",
            "Epoch: 286, Loss: 0.707612196081563\n",
            "Epoch: 287, Loss: 0.705434284712139\n",
            "Epoch: 288, Loss: 0.7032759973877355\n",
            "Epoch: 289, Loss: 0.7011364698410034\n",
            "Epoch: 290, Loss: 0.6990162996869338\n",
            "Epoch: 291, Loss: 0.696914686968452\n",
            "Epoch: 292, Loss: 0.6948251645816\n",
            "Epoch: 293, Loss: 0.6927517621140731\n",
            "Epoch: 294, Loss: 0.6906964637731251\n",
            "Epoch: 295, Loss: 0.6886613023908514\n",
            "Epoch: 296, Loss: 0.6866438812331149\n",
            "Epoch: 297, Loss: 0.6846431995693006\n",
            "Epoch: 298, Loss: 0.6826607679065905\n",
            "Epoch: 299, Loss: 0.6806936248352653\n",
            "Epoch: 300, Loss: 0.6787426346226743\n",
            "Epoch: 301, Loss: 0.6768090913170263\n",
            "Epoch: 302, Loss: 0.6748901715404109\n",
            "Epoch: 303, Loss: 0.67298539845567\n",
            "Epoch: 304, Loss: 0.671098026790117\n",
            "Epoch: 305, Loss: 0.6692260143003965\n",
            "Epoch: 306, Loss: 0.6673656905952253\n",
            "Epoch: 307, Loss: 0.6655132833280062\n",
            "Epoch: 308, Loss: 0.6636736000839033\n",
            "Epoch: 309, Loss: 0.6618496948166898\n",
            "Epoch: 310, Loss: 0.6600409024640134\n",
            "Epoch: 311, Loss: 0.658246077989277\n",
            "Epoch: 312, Loss: 0.656466156244278\n",
            "Epoch: 313, Loss: 0.6547004956948129\n",
            "Epoch: 314, Loss: 0.6529482038397538\n",
            "Epoch: 315, Loss: 0.6512080713322288\n",
            "Epoch: 316, Loss: 0.649480363256053\n",
            "Epoch: 317, Loss: 0.6477657305566888\n",
            "Epoch: 318, Loss: 0.6460646092891693\n",
            "Epoch: 319, Loss: 0.6443775578549034\n",
            "Epoch: 320, Loss: 0.6427029951622611\n",
            "Epoch: 321, Loss: 0.6410413980484009\n",
            "Epoch: 322, Loss: 0.6393884138057107\n",
            "Epoch: 323, Loss: 0.6377473002985904\n",
            "Epoch: 324, Loss: 0.636117979099876\n",
            "Epoch: 325, Loss: 0.6344981868016092\n",
            "Epoch: 326, Loss: 0.6328871061927394\n",
            "Epoch: 327, Loss: 0.6312853458680605\n",
            "Epoch: 328, Loss: 0.6296931960080799\n",
            "Epoch: 329, Loss: 0.6281108542492515\n",
            "Epoch: 330, Loss: 0.6265388695817244\n",
            "Epoch: 331, Loss: 0.6249781674460361\n",
            "Epoch: 332, Loss: 0.6234287729388789\n",
            "Epoch: 333, Loss: 0.6218911064298529\n",
            "Epoch: 334, Loss: 0.6203625515887612\n",
            "Epoch: 335, Loss: 0.6188453953517111\n",
            "Epoch: 336, Loss: 0.6173361022221414\n",
            "Epoch: 337, Loss: 0.6158353011859091\n",
            "Epoch: 338, Loss: 0.614344995272787\n",
            "Epoch: 339, Loss: 0.6128644119752081\n",
            "Epoch: 340, Loss: 0.6113933426769156\n",
            "Epoch: 341, Loss: 0.6099340790196469\n",
            "Epoch: 342, Loss: 0.608485495573596\n",
            "Epoch: 343, Loss: 0.6070453257937181\n",
            "Epoch: 344, Loss: 0.6056150958726281\n",
            "Epoch: 345, Loss: 0.6041916640181291\n",
            "Epoch: 346, Loss: 0.6027740883199793\n",
            "Epoch: 347, Loss: 0.6013634495045009\n",
            "Epoch: 348, Loss: 0.599961654920327\n",
            "Epoch: 349, Loss: 0.5985698817591918\n",
            "Epoch: 350, Loss: 0.5971860964047281\n",
            "Epoch: 351, Loss: 0.5958107615772047\n",
            "Epoch: 352, Loss: 0.594445106230284\n",
            "Epoch: 353, Loss: 0.5930853611544559\n",
            "Epoch: 354, Loss: 0.5917326894245649\n",
            "Epoch: 355, Loss: 0.5903877890423724\n",
            "Epoch: 356, Loss: 0.5890491934199082\n",
            "Epoch: 357, Loss: 0.58771828679662\n",
            "Epoch: 358, Loss: 0.5863964643917585\n",
            "Epoch: 359, Loss: 0.5850836109173926\n",
            "Epoch: 360, Loss: 0.5837806031892174\n",
            "Epoch: 361, Loss: 0.5824844131344243\n",
            "Epoch: 362, Loss: 0.5811962461785266\n",
            "Epoch: 363, Loss: 0.5799155133335214\n",
            "Epoch: 364, Loss: 0.5786414703256205\n",
            "Epoch: 365, Loss: 0.5773760629327673\n",
            "Epoch: 366, Loss: 0.5761182606220245\n",
            "Epoch: 367, Loss: 0.5748673646073592\n",
            "Epoch: 368, Loss: 0.5736221671104431\n",
            "Epoch: 369, Loss: 0.5723826030367299\n",
            "Epoch: 370, Loss: 0.5711489939375928\n",
            "Epoch: 371, Loss: 0.5699218095917451\n",
            "Epoch: 372, Loss: 0.5686990869672675\n",
            "Epoch: 373, Loss: 0.5674819091432973\n",
            "Epoch: 374, Loss: 0.5662708274627987\n",
            "Epoch: 375, Loss: 0.5650674732107865\n",
            "Epoch: 376, Loss: 0.5638681861915087\n",
            "Epoch: 377, Loss: 0.562675510582171\n",
            "Epoch: 378, Loss: 0.5614880433208064\n",
            "Epoch: 379, Loss: 0.5603049452367582\n",
            "Epoch: 380, Loss: 0.5591259818328055\n",
            "Epoch: 381, Loss: 0.5579533161301362\n",
            "Epoch: 382, Loss: 0.5567873355589414\n",
            "Epoch: 383, Loss: 0.5556270189975437\n",
            "Epoch: 384, Loss: 0.5544734103114981\n",
            "Epoch: 385, Loss: 0.5533231959531182\n",
            "Epoch: 386, Loss: 0.5521786683484128\n",
            "Epoch: 387, Loss: 0.5510392879184923\n",
            "Epoch: 388, Loss: 0.5499081235182913\n",
            "Epoch: 389, Loss: 0.5487812671222185\n",
            "Epoch: 390, Loss: 0.5476590512614501\n",
            "Epoch: 391, Loss: 0.5465431566301145\n",
            "Epoch: 392, Loss: 0.5454328060150146\n",
            "Epoch: 393, Loss: 0.5443276935502103\n",
            "Epoch: 394, Loss: 0.5432258405183491\n",
            "Epoch: 395, Loss: 0.5421289111438551\n",
            "Epoch: 396, Loss: 0.5410364097670505\n",
            "Epoch: 397, Loss: 0.539949219477804\n",
            "Epoch: 398, Loss: 0.5388663756219965\n",
            "Epoch: 399, Loss: 0.5377891596994901\n",
            "Epoch: 400, Loss: 0.536718697924363\n",
            "Epoch: 401, Loss: 0.5356536656618118\n",
            "Epoch: 402, Loss: 0.5345935986230248\n",
            "Epoch: 403, Loss: 0.5335388418875242\n",
            "Epoch: 404, Loss: 0.5324885060912684\n",
            "Epoch: 405, Loss: 0.5314431692424574\n",
            "Epoch: 406, Loss: 0.5304018666869715\n",
            "Epoch: 407, Loss: 0.5293644619615454\n",
            "Epoch: 408, Loss: 0.5283307762522447\n",
            "Epoch: 409, Loss: 0.5273015397159677\n",
            "Epoch: 410, Loss: 0.5262778330790369\n",
            "Epoch: 411, Loss: 0.5252572447061539\n",
            "Epoch: 412, Loss: 0.5242428136499304\n",
            "Epoch: 413, Loss: 0.5232308255998712\n",
            "Epoch: 414, Loss: 0.5222218679754358\n",
            "Epoch: 415, Loss: 0.5212162960516779\n",
            "Epoch: 416, Loss: 0.520215744250699\n",
            "Epoch: 417, Loss: 0.5192183624756964\n",
            "Epoch: 418, Loss: 0.5182251953764966\n",
            "Epoch: 419, Loss: 0.5172372774073952\n",
            "Epoch: 420, Loss: 0.5162531568815834\n",
            "Epoch: 421, Loss: 0.5152736423831237\n",
            "Epoch: 422, Loss: 0.5142973959445953\n",
            "Epoch: 423, Loss: 0.5133261210040042\n",
            "Epoch: 424, Loss: 0.5123589964289414\n",
            "Epoch: 425, Loss: 0.5113967461021323\n",
            "Epoch: 426, Loss: 0.5104387386849052\n",
            "Epoch: 427, Loss: 0.5094840777547736\n",
            "Epoch: 428, Loss: 0.5085338597234926\n",
            "Epoch: 429, Loss: 0.5075856337421819\n",
            "Epoch: 430, Loss: 0.5066419557521218\n",
            "Epoch: 431, Loss: 0.5057023873454646\n",
            "Epoch: 432, Loss: 0.5047663356128492\n",
            "Epoch: 433, Loss: 0.5038349699032935\n",
            "Epoch: 434, Loss: 0.5029055680099287\n",
            "Epoch: 435, Loss: 0.5019808090046832\n",
            "Epoch: 436, Loss: 0.5010583377198169\n",
            "Epoch: 437, Loss: 0.500140907733064\n",
            "Epoch: 438, Loss: 0.49922618348347514\n",
            "Epoch: 439, Loss: 0.49831405674156387\n",
            "Epoch: 440, Loss: 0.4974060944820705\n",
            "Epoch: 441, Loss: 0.4964993086300398\n",
            "Epoch: 442, Loss: 0.49559687626989263\n",
            "Epoch: 443, Loss: 0.49469755982097824\n",
            "Epoch: 444, Loss: 0.4938019796421653\n",
            "Epoch: 445, Loss: 0.4929090401059703\n",
            "Epoch: 446, Loss: 0.4920194745063782\n",
            "Epoch: 447, Loss: 0.49113394555292633\n",
            "Epoch: 448, Loss: 0.49024990122569234\n",
            "Epoch: 449, Loss: 0.48936957826739863\n",
            "Epoch: 450, Loss: 0.48849281511808695\n",
            "Epoch: 451, Loss: 0.4876186479079096\n",
            "Epoch: 452, Loss: 0.48674830715907247\n",
            "Epoch: 453, Loss: 0.48587916791439056\n",
            "Epoch: 454, Loss: 0.48501245442189667\n",
            "Epoch: 455, Loss: 0.48414759886892217\n",
            "Epoch: 456, Loss: 0.48328569688295064\n",
            "Epoch: 457, Loss: 0.48242615790743576\n",
            "Epoch: 458, Loss: 0.4815675835860403\n",
            "Epoch: 459, Loss: 0.4807111738543761\n",
            "Epoch: 460, Loss: 0.47985562133161647\n",
            "Epoch: 461, Loss: 0.4790021581085105\n",
            "Epoch: 462, Loss: 0.4781522280291507\n",
            "Epoch: 463, Loss: 0.4773038970796685\n",
            "Epoch: 464, Loss: 0.47645750249686997\n",
            "Epoch: 465, Loss: 0.47561516495127426\n",
            "Epoch: 466, Loss: 0.4747742994835502\n",
            "Epoch: 467, Loss: 0.4739360150537993\n",
            "Epoch: 468, Loss: 0.4731010339762035\n",
            "Epoch: 469, Loss: 0.4722657752664466\n",
            "Epoch: 470, Loss: 0.47143456104554626\n",
            "Epoch: 471, Loss: 0.47060650116518926\n",
            "Epoch: 472, Loss: 0.46978093997428294\n",
            "Epoch: 473, Loss: 0.46895853704527807\n",
            "Epoch: 474, Loss: 0.46813848144129705\n",
            "Epoch: 475, Loss: 0.4673216907601607\n",
            "Epoch: 476, Loss: 0.46650739092575877\n",
            "Epoch: 477, Loss: 0.46569481099906723\n",
            "Epoch: 478, Loss: 0.4648826326194562\n",
            "Epoch: 479, Loss: 0.46407177495329005\n",
            "Epoch: 480, Loss: 0.46326333754941035\n",
            "Epoch: 481, Loss: 0.462456223996062\n",
            "Epoch: 482, Loss: 0.46165155893877935\n",
            "Epoch: 483, Loss: 0.4608487973087712\n",
            "Epoch: 484, Loss: 0.4600496488182168\n",
            "Epoch: 485, Loss: 0.45925233081767436\n",
            "Epoch: 486, Loss: 0.4584578110983497\n",
            "Epoch: 487, Loss: 0.45766615710760417\n",
            "Epoch: 488, Loss: 0.4568758763765034\n",
            "Epoch: 489, Loss: 0.4560871461504384\n",
            "Epoch: 490, Loss: 0.45530076638648387\n",
            "Epoch: 491, Loss: 0.4545156649853054\n",
            "Epoch: 492, Loss: 0.45373525509708806\n",
            "Epoch: 493, Loss: 0.4529583540401961\n",
            "Epoch: 494, Loss: 0.45218103418224737\n",
            "Epoch: 495, Loss: 0.45140635496691656\n",
            "Epoch: 496, Loss: 0.45063311645859166\n",
            "Epoch: 497, Loss: 0.4498606088914369\n",
            "Epoch: 498, Loss: 0.44908873266295385\n",
            "Epoch: 499, Loss: 0.44831729013668864\n",
            "Epoch: 500, Loss: 0.44754648522326823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKEctmdHb_VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = []\n",
        "for i, (X, y) in enumerate(test_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    X = X.to(device)\n",
        "    output = model(X)   \n",
        "    test_outputs.append(output.cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nIz1r5mcAuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs_converted = []\n",
        "for i in range(len(test_outputs)):\n",
        "  current = test_outputs[i][0]\n",
        "  add = [1 if x >= 0.50 else 0 for x in current]\n",
        "  test_outputs_converted.append(add)\n",
        "test_outputs_converted = np.array(test_outputs_converted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZOUB5lvcGNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb48377d-439f-439a-cf3d-ae94c9254b19"
      },
      "source": [
        "accuracy = 0\n",
        "missed = []\n",
        "for i in range(len(test_outputs_converted)):\n",
        "  if collections.Counter(test_outputs_converted[i]) == collections.Counter(converted_test[i]):\n",
        "    accuracy += 1\n",
        "  else:\n",
        "    missed.append(i)\n",
        "print(f'Testing Accuracy: {accuracy/33}')"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.8181818181818182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwMTkukweHSk",
        "colab_type": "text"
      },
      "source": [
        "# Exploring other feature selection/reduction techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBEzuAWWngmT",
        "colab_type": "text"
      },
      "source": [
        "# Sparse PCA: PCA with L1 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXbCehDQmmsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 2\n",
        "sparse_pca = MiniBatchSparsePCA(n_components = n).fit(train_scaled)\n",
        "sparse_train_genes = sparse_pca.transform(train_scaled)\n",
        "sparse_test_genes = sparse_pca.transform(test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uMHnuIT36ov",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the best model on these genes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii_WtBDB36Iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8065b1b-ccf9-4967-ff81-51c56d228a5f"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid.fit(np.array(sparse_train_genes), np.array(mapped_labels_train))\n",
        "ridge_grid.best_score_"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6578160919540229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zov_InGd4GM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07509ee1-9722-4657-ee6e-90485a406fc9"
      },
      "source": [
        "ridge_grid.score(np.array(sparse_test_genes), np.array(mapped_labels_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.696969696969697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghNkQfQh4pRz",
        "colab_type": "text"
      },
      "source": [
        "Using the genes determined to be the most important by Sparse PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pEKr7vt4mYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "03b688cc-19b5-47b8-bcd5-4f3ce90b5a7d"
      },
      "source": [
        "all_weights = pd.DataFrame(sparse_pca.components_)\n",
        "col_names = train_data.columns.values\n",
        "all_weights.columns = col_names\n",
        "all_weights.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>15</th>\n",
              "      <th>28</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>47</th>\n",
              "      <th>50</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>82</th>\n",
              "      <th>86</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>96</th>\n",
              "      <th>98</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>...</th>\n",
              "      <th>24150</th>\n",
              "      <th>24155</th>\n",
              "      <th>24156</th>\n",
              "      <th>24160</th>\n",
              "      <th>24163</th>\n",
              "      <th>24164</th>\n",
              "      <th>24165</th>\n",
              "      <th>24168</th>\n",
              "      <th>24184</th>\n",
              "      <th>24191</th>\n",
              "      <th>24202</th>\n",
              "      <th>24203</th>\n",
              "      <th>24207</th>\n",
              "      <th>24214</th>\n",
              "      <th>24215</th>\n",
              "      <th>24220</th>\n",
              "      <th>24223</th>\n",
              "      <th>24224</th>\n",
              "      <th>24237</th>\n",
              "      <th>24244</th>\n",
              "      <th>24253</th>\n",
              "      <th>24255</th>\n",
              "      <th>24256</th>\n",
              "      <th>24260</th>\n",
              "      <th>24261</th>\n",
              "      <th>24271</th>\n",
              "      <th>24272</th>\n",
              "      <th>24273</th>\n",
              "      <th>24277</th>\n",
              "      <th>24283</th>\n",
              "      <th>24285</th>\n",
              "      <th>24288</th>\n",
              "      <th>24291</th>\n",
              "      <th>24296</th>\n",
              "      <th>24308</th>\n",
              "      <th>24309</th>\n",
              "      <th>24313</th>\n",
              "      <th>24316</th>\n",
              "      <th>24317</th>\n",
              "      <th>24323</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.014237</td>\n",
              "      <td>0.014525</td>\n",
              "      <td>0.00199</td>\n",
              "      <td>0.020130</td>\n",
              "      <td>0.010377</td>\n",
              "      <td>0.005744</td>\n",
              "      <td>-0.006668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002141</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>0.013758</td>\n",
              "      <td>0.019174</td>\n",
              "      <td>0.014932</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>0.012858</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.002698</td>\n",
              "      <td>0.013339</td>\n",
              "      <td>0.002816</td>\n",
              "      <td>0.013898</td>\n",
              "      <td>-0.001266</td>\n",
              "      <td>0.012467</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.011132</td>\n",
              "      <td>0.004317</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.010167</td>\n",
              "      <td>0.009958</td>\n",
              "      <td>0.015693</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>-0.004169</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.003233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019874</td>\n",
              "      <td>0.019304</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019547</td>\n",
              "      <td>-0.020863</td>\n",
              "      <td>-0.017228</td>\n",
              "      <td>-0.019651</td>\n",
              "      <td>-0.020818</td>\n",
              "      <td>-0.020224</td>\n",
              "      <td>-0.023864</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>-0.000700</td>\n",
              "      <td>-0.005823</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>-0.00586</td>\n",
              "      <td>-0.009581</td>\n",
              "      <td>0.007015</td>\n",
              "      <td>-0.003814</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.019958</td>\n",
              "      <td>-0.005933</td>\n",
              "      <td>-0.014645</td>\n",
              "      <td>-0.009262</td>\n",
              "      <td>-0.017239</td>\n",
              "      <td>-0.011472</td>\n",
              "      <td>-0.023766</td>\n",
              "      <td>-0.005172</td>\n",
              "      <td>0.018053</td>\n",
              "      <td>-0.002095</td>\n",
              "      <td>-0.007080</td>\n",
              "      <td>0.018804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.000664</td>\n",
              "      <td>0.004101</td>\n",
              "      <td>0.006252</td>\n",
              "      <td>-0.017616</td>\n",
              "      <td>-0.011084</td>\n",
              "      <td>-0.011202</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>-0.005465</td>\n",
              "      <td>-0.015299</td>\n",
              "      <td>0.006110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.000947</td>\n",
              "      <td>-0.011221</td>\n",
              "      <td>0.01426</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.006500</td>\n",
              "      <td>0.021122</td>\n",
              "      <td>0.019346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.006068</td>\n",
              "      <td>0.013138</td>\n",
              "      <td>0.017092</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.013102</td>\n",
              "      <td>-0.013427</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>-0.016135</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>0.019441</td>\n",
              "      <td>0.005754</td>\n",
              "      <td>-0.013633</td>\n",
              "      <td>0.006786</td>\n",
              "      <td>0.01392</td>\n",
              "      <td>0.013473</td>\n",
              "      <td>0.010995</td>\n",
              "      <td>0.013328</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.011532</td>\n",
              "      <td>0.017061</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018791</td>\n",
              "      <td>0.014257</td>\n",
              "      <td>-0.007448</td>\n",
              "      <td>0.002605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>-0.006384</td>\n",
              "      <td>-0.001957</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>0.006478</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>-0.001113</td>\n",
              "      <td>0.010388</td>\n",
              "      <td>0.006236</td>\n",
              "      <td>-0.003752</td>\n",
              "      <td>0.00758</td>\n",
              "      <td>0.009606</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.009211</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006439</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.012746</td>\n",
              "      <td>-0.003252</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.013623</td>\n",
              "      <td>-0.010305</td>\n",
              "      <td>0.008567</td>\n",
              "      <td>-0.006604</td>\n",
              "      <td>0.002302</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>0.002966</td>\n",
              "      <td>0.010434</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009776</td>\n",
              "      <td>0.004584</td>\n",
              "      <td>-0.014265</td>\n",
              "      <td>0.010494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.016129</td>\n",
              "      <td>-0.017821</td>\n",
              "      <td>-0.020634</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows  8675 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         2        4  ...     24316     24317     24323\n",
              "0  0.014237  0.014525  0.00199  ... -0.005465 -0.015299  0.006110\n",
              "1 -0.000947 -0.011221  0.01426  ... -0.016129 -0.017821 -0.020634\n",
              "\n",
              "[2 rows x 8675 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhoHeVgN44tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sparse_important_genes = []\n",
        "temp = pd.DataFrame(np.abs(all_weights)).T\n",
        "for i in range(n):\n",
        "    sparse_important_genes.extend(temp.nlargest(100, i).index.values)\n",
        "important_genes = np.unique(sparse_important_genes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ht73uCz49jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sparse_important_train = train_data[sparse_important_genes]\n",
        "sparse_important_test = test_data[sparse_important_genes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSUfDmG25EDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb390ea8-60d3-438d-8f0d-d8367702933e"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid_sparse = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid_sparse.fit(np.array(sparse_important_train), np.array(mapped_labels_train))\n",
        "ridge_grid_sparse.best_score_"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6718390804597701"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykQ6dF1L5JIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bd28d0e-f8d5-4e5e-bc82-1f0b10e1084c"
      },
      "source": [
        "ridge_grid.score(np.array(sparse_important_test), np.array(mapped_labels_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7575757575757576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ts_PtZ-PctO",
        "colab_type": "text"
      },
      "source": [
        "# Feature selection on the sparse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pWH335VPzi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi = permutation_importance(ridge_grid_sparse.best_estimator_, sparse_important_train, mapped_labels_train, n_repeats=10, random_state=0)\n",
        "pi_mean_vals = pi['importances_mean']\n",
        "cols = pd.DataFrame(sparse_important_train).columns\n",
        "pi_importances_pairs = zip(cols, pi_mean_vals)\n",
        "pi_pairs_sorted = sorted(pi_importances_pairs, key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuzLQbcQQtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_zero_sparse = [int(x[0]) for x in pi_pairs_sorted if x[1] > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsgBbyZjR3Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selector = RFE(ridge_grid_sparse.best_estimator_, 1, step=1)\n",
        "selector = selector.fit(sparse_important_train, mapped_labels_train)\n",
        "ranking_pairs = zip(pd.DataFrame(sparse_important_train).columns, selector.ranking_)\n",
        "ranking_pairs_sorted = sorted(ranking_pairs, key=lambda x: x[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1mFsD5fR_GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_100_rfe_sparse = []\n",
        "for i in range(100):\n",
        "  top_100_rfe_sparse.append(int(ranking_pairs_sorted[i][0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XewOPkaN8skX",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network for sparse PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88DNMoMP_QFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_target_tensor = torch.tensor(converted_train)\n",
        "test_target_tensor = torch.tensor(converted_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_dmpVsW8zJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tensor = torch.tensor(sparse_important_train.values.astype(np.float32)) \n",
        "test_tensor = torch.tensor(sparse_important_test.values.astype(np.float32))\n",
        "batch_size = 16\n",
        "# Define datasets and dataloaders\n",
        "testing_dataset = TensorDataset(test_tensor, test_target_tensor)\n",
        "training_dataset = TensorDataset(train_tensor, train_target_tensor)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size = 1)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size = batch_size )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApCPd0Z_cDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class rnn_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(rnn_model, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(len(sparse_important_train.columns), 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 3),\n",
        "            # Sigmoid funtion on each of the output nodes\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # The last layer hidden states from BERT are used as embeddings/semantic \n",
        "    # representation for the input and therefore, passed into the MLP\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQmV3Kn2_m9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile and train model\n",
        "model = rnn_model()\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcfV21P4_uSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10b3e11f-eaa2-49a3-d9be-f86a9e96187a"
      },
      "source": [
        "epochs = 500\n",
        "for e in range(0, epochs):\n",
        "    model.train()\n",
        "    current_loss = 0\n",
        "    for i, (X, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # To GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device).double()\n",
        "        output = model(X).double()\n",
        "        loss = loss_function(output, y)\n",
        "        loss = loss.type(torch.cuda.FloatTensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'Epoch: {e+1}, Loss: {current_loss/len(train_dataloader)}')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 2.085032394057826\n",
            "Epoch: 2, Loss: 2.0490491954903853\n",
            "Epoch: 3, Loss: 2.0117379803406563\n",
            "Epoch: 4, Loss: 1.9732023979488171\n",
            "Epoch: 5, Loss: 1.934707854923449\n",
            "Epoch: 6, Loss: 1.8973391495252911\n",
            "Epoch: 7, Loss: 1.8607337412081266\n",
            "Epoch: 8, Loss: 1.8239907277257819\n",
            "Epoch: 9, Loss: 1.7876568279768292\n",
            "Epoch: 10, Loss: 1.7524298366747404\n",
            "Epoch: 11, Loss: 1.7185349025224383\n",
            "Epoch: 12, Loss: 1.6869918296211643\n",
            "Epoch: 13, Loss: 1.6579956631911428\n",
            "Epoch: 14, Loss: 1.6310610582954006\n",
            "Epoch: 15, Loss: 1.6061146886725175\n",
            "Epoch: 16, Loss: 1.5831599110051204\n",
            "Epoch: 17, Loss: 1.5619800467240184\n",
            "Epoch: 18, Loss: 1.5422855050940263\n",
            "Epoch: 19, Loss: 1.5239977334675037\n",
            "Epoch: 20, Loss: 1.5069608249162372\n",
            "Epoch: 21, Loss: 1.4910249647341276\n",
            "Epoch: 22, Loss: 1.4760495737979287\n",
            "Epoch: 23, Loss: 1.461932383085552\n",
            "Epoch: 24, Loss: 1.4485766887664795\n",
            "Epoch: 25, Loss: 1.4359307791057385\n",
            "Epoch: 26, Loss: 1.4238866693095158\n",
            "Epoch: 27, Loss: 1.4124288182509572\n",
            "Epoch: 28, Loss: 1.4016022180256091\n",
            "Epoch: 29, Loss: 1.3912399693539268\n",
            "Epoch: 30, Loss: 1.3813673759761609\n",
            "Epoch: 31, Loss: 1.371906838918987\n",
            "Epoch: 32, Loss: 1.3627636056197316\n",
            "Epoch: 33, Loss: 1.3540414697245549\n",
            "Epoch: 34, Loss: 1.3457352926856594\n",
            "Epoch: 35, Loss: 1.3378342954736007\n",
            "Epoch: 36, Loss: 1.3303291044737164\n",
            "Epoch: 37, Loss: 1.3231470271160728\n",
            "Epoch: 38, Loss: 1.3163043825249923\n",
            "Epoch: 39, Loss: 1.3097596482226723\n",
            "Epoch: 40, Loss: 1.30348759575894\n",
            "Epoch: 41, Loss: 1.2975057990927445\n",
            "Epoch: 42, Loss: 1.2917701382386058\n",
            "Epoch: 43, Loss: 1.2862646140550311\n",
            "Epoch: 44, Loss: 1.2809516567932933\n",
            "Epoch: 45, Loss: 1.2757941108000905\n",
            "Epoch: 46, Loss: 1.2707644638262297\n",
            "Epoch: 47, Loss: 1.2658307113145526\n",
            "Epoch: 48, Loss: 1.260972211235448\n",
            "Epoch: 49, Loss: 1.2562359508715177\n",
            "Epoch: 50, Loss: 1.25160088664607\n",
            "Epoch: 51, Loss: 1.2470564465773732\n",
            "Epoch: 52, Loss: 1.2426013381857621\n",
            "Epoch: 53, Loss: 1.2382384915100901\n",
            "Epoch: 54, Loss: 1.2339551950755872\n",
            "Epoch: 55, Loss: 1.2296869190115678\n",
            "Epoch: 56, Loss: 1.2254218176791543\n",
            "Epoch: 57, Loss: 1.2211991924988597\n",
            "Epoch: 58, Loss: 1.216960913256595\n",
            "Epoch: 59, Loss: 1.212691489018892\n",
            "Epoch: 60, Loss: 1.208390944882443\n",
            "Epoch: 61, Loss: 1.2040629386901855\n",
            "Epoch: 62, Loss: 1.1997502477545487\n",
            "Epoch: 63, Loss: 1.1954980461220992\n",
            "Epoch: 64, Loss: 1.1912820464686344\n",
            "Epoch: 65, Loss: 1.1871128646950972\n",
            "Epoch: 66, Loss: 1.18299989323867\n",
            "Epoch: 67, Loss: 1.178941155734815\n",
            "Epoch: 68, Loss: 1.1749278055994135\n",
            "Epoch: 69, Loss: 1.1709495908335636\n",
            "Epoch: 70, Loss: 1.1670156842783879\n",
            "Epoch: 71, Loss: 1.1631081417987221\n",
            "Epoch: 72, Loss: 1.1592580080032349\n",
            "Epoch: 73, Loss: 1.155444389895389\n",
            "Epoch: 74, Loss: 1.1516749231438888\n",
            "Epoch: 75, Loss: 1.1479446072327464\n",
            "Epoch: 76, Loss: 1.144246358620493\n",
            "Epoch: 77, Loss: 1.1405906677246094\n",
            "Epoch: 78, Loss: 1.1369715301614058\n",
            "Epoch: 79, Loss: 1.1333851124110974\n",
            "Epoch: 80, Loss: 1.1298297894628424\n",
            "Epoch: 81, Loss: 1.1262954473495483\n",
            "Epoch: 82, Loss: 1.1227842443867733\n",
            "Epoch: 83, Loss: 1.1192881747295982\n",
            "Epoch: 84, Loss: 1.1158174244981063\n",
            "Epoch: 85, Loss: 1.1123691765885604\n",
            "Epoch: 86, Loss: 1.108944993270071\n",
            "Epoch: 87, Loss: 1.1055196084474261\n",
            "Epoch: 88, Loss: 1.1020991802215576\n",
            "Epoch: 89, Loss: 1.0987044478717602\n",
            "Epoch: 90, Loss: 1.095333297001688\n",
            "Epoch: 91, Loss: 1.0919943640106602\n",
            "Epoch: 92, Loss: 1.0886877743821395\n",
            "Epoch: 93, Loss: 1.0853720432833622\n",
            "Epoch: 94, Loss: 1.0820640325546265\n",
            "Epoch: 95, Loss: 1.078782138071562\n",
            "Epoch: 96, Loss: 1.0755373898305391\n",
            "Epoch: 97, Loss: 1.0723040668587935\n",
            "Epoch: 98, Loss: 1.069086984584206\n",
            "Epoch: 99, Loss: 1.0658907827578092\n",
            "Epoch: 100, Loss: 1.062700983725096\n",
            "Epoch: 101, Loss: 1.059539092214484\n",
            "Epoch: 102, Loss: 1.0563999979119552\n",
            "Epoch: 103, Loss: 1.0532844317586798\n",
            "Epoch: 104, Loss: 1.0502024606654519\n",
            "Epoch: 105, Loss: 1.0471459533038892\n",
            "Epoch: 106, Loss: 1.044142694849717\n",
            "Epoch: 107, Loss: 1.041169655950446\n",
            "Epoch: 108, Loss: 1.0382421863706488\n",
            "Epoch: 109, Loss: 1.0353458837458962\n",
            "Epoch: 110, Loss: 1.0324823730870296\n",
            "Epoch: 111, Loss: 1.0296622295128672\n",
            "Epoch: 112, Loss: 1.0268745987038863\n",
            "Epoch: 113, Loss: 1.024123982379311\n",
            "Epoch: 114, Loss: 1.0214223579356545\n",
            "Epoch: 115, Loss: 1.0187576005333348\n",
            "Epoch: 116, Loss: 1.0161196934549432\n",
            "Epoch: 117, Loss: 1.0135260098858883\n",
            "Epoch: 118, Loss: 1.0109632172082599\n",
            "Epoch: 119, Loss: 1.0084235291731984\n",
            "Epoch: 120, Loss: 1.0059254734139693\n",
            "Epoch: 121, Loss: 1.0034719517356472\n",
            "Epoch: 122, Loss: 1.001039401481026\n",
            "Epoch: 123, Loss: 0.9986431535921598\n",
            "Epoch: 124, Loss: 0.9962864925986842\n",
            "Epoch: 125, Loss: 0.9939597092176738\n",
            "Epoch: 126, Loss: 0.9916647233461079\n",
            "Epoch: 127, Loss: 0.9893948247558192\n",
            "Epoch: 128, Loss: 0.9871696672941509\n",
            "Epoch: 129, Loss: 0.9849758587385479\n",
            "Epoch: 130, Loss: 0.9828076080272072\n",
            "Epoch: 131, Loss: 0.980678423454887\n",
            "Epoch: 132, Loss: 0.9785639329960472\n",
            "Epoch: 133, Loss: 0.9764935813452068\n",
            "Epoch: 134, Loss: 0.9744484832412318\n",
            "Epoch: 135, Loss: 0.9724378052510714\n",
            "Epoch: 136, Loss: 0.970463542561782\n",
            "Epoch: 137, Loss: 0.968518034407967\n",
            "Epoch: 138, Loss: 0.9665907495900204\n",
            "Epoch: 139, Loss: 0.9646893582845989\n",
            "Epoch: 140, Loss: 0.9628254018331829\n",
            "Epoch: 141, Loss: 0.9609876400546024\n",
            "Epoch: 142, Loss: 0.9591756933613828\n",
            "Epoch: 143, Loss: 0.9574065522143715\n",
            "Epoch: 144, Loss: 0.9556489775055333\n",
            "Epoch: 145, Loss: 0.9539140776584023\n",
            "Epoch: 146, Loss: 0.95221220505865\n",
            "Epoch: 147, Loss: 0.9505245120901811\n",
            "Epoch: 148, Loss: 0.948860620197497\n",
            "Epoch: 149, Loss: 0.947226816102078\n",
            "Epoch: 150, Loss: 0.9456201070233395\n",
            "Epoch: 151, Loss: 0.9440484580240751\n",
            "Epoch: 152, Loss: 0.9424923783854434\n",
            "Epoch: 153, Loss: 0.940968111941689\n",
            "Epoch: 154, Loss: 0.9394667493669611\n",
            "Epoch: 155, Loss: 0.9379906905324835\n",
            "Epoch: 156, Loss: 0.936540446783367\n",
            "Epoch: 157, Loss: 0.9351077079772949\n",
            "Epoch: 158, Loss: 0.933688948028966\n",
            "Epoch: 159, Loss: 0.9322932080218667\n",
            "Epoch: 160, Loss: 0.9309272546517221\n",
            "Epoch: 161, Loss: 0.9295718230699238\n",
            "Epoch: 162, Loss: 0.9282661299956473\n",
            "Epoch: 163, Loss: 0.9269446228679857\n",
            "Epoch: 164, Loss: 0.9256557320293627\n",
            "Epoch: 165, Loss: 0.9243956835646379\n",
            "Epoch: 166, Loss: 0.9231431201884621\n",
            "Epoch: 167, Loss: 0.9219025373458862\n",
            "Epoch: 168, Loss: 0.9207005783131248\n",
            "Epoch: 169, Loss: 0.9195044479872051\n",
            "Epoch: 170, Loss: 0.9183236078212136\n",
            "Epoch: 171, Loss: 0.917159660866386\n",
            "Epoch: 172, Loss: 0.9160160547808597\n",
            "Epoch: 173, Loss: 0.9148913684644198\n",
            "Epoch: 174, Loss: 0.9137722065574244\n",
            "Epoch: 175, Loss: 0.9126719487340826\n",
            "Epoch: 176, Loss: 0.9115990180718271\n",
            "Epoch: 177, Loss: 0.9105137680706225\n",
            "Epoch: 178, Loss: 0.9094482534810117\n",
            "Epoch: 179, Loss: 0.9083970785140991\n",
            "Epoch: 180, Loss: 0.9073740714474728\n",
            "Epoch: 181, Loss: 0.906356554282339\n",
            "Epoch: 182, Loss: 0.9053705522888585\n",
            "Epoch: 183, Loss: 0.9044152466874373\n",
            "Epoch: 184, Loss: 0.9034766335236398\n",
            "Epoch: 185, Loss: 0.902557655384666\n",
            "Epoch: 186, Loss: 0.9016488257207369\n",
            "Epoch: 187, Loss: 0.9007698172017148\n",
            "Epoch: 188, Loss: 0.8998977698777851\n",
            "Epoch: 189, Loss: 0.8990277742084704\n",
            "Epoch: 190, Loss: 0.8981719362108331\n",
            "Epoch: 191, Loss: 0.8973215943888614\n",
            "Epoch: 192, Loss: 0.8965053087786624\n",
            "Epoch: 193, Loss: 0.8956781688489412\n",
            "Epoch: 194, Loss: 0.8948735218299063\n",
            "Epoch: 195, Loss: 0.8940796601144891\n",
            "Epoch: 196, Loss: 0.8932904789322301\n",
            "Epoch: 197, Loss: 0.8925199759633917\n",
            "Epoch: 198, Loss: 0.8917411001105058\n",
            "Epoch: 199, Loss: 0.8909775928447121\n",
            "Epoch: 200, Loss: 0.8902123695925662\n",
            "Epoch: 201, Loss: 0.8894753079665335\n",
            "Epoch: 202, Loss: 0.8887378871440887\n",
            "Epoch: 203, Loss: 0.8880286452017332\n",
            "Epoch: 204, Loss: 0.8873285290442015\n",
            "Epoch: 205, Loss: 0.8866283893585205\n",
            "Epoch: 206, Loss: 0.8859580817975496\n",
            "Epoch: 207, Loss: 0.8853038800390143\n",
            "Epoch: 208, Loss: 0.8846459749497866\n",
            "Epoch: 209, Loss: 0.8840073660800332\n",
            "Epoch: 210, Loss: 0.8833783959087572\n",
            "Epoch: 211, Loss: 0.8827562959570634\n",
            "Epoch: 212, Loss: 0.8821498767325753\n",
            "Epoch: 213, Loss: 0.8815447076370841\n",
            "Epoch: 214, Loss: 0.8809475726202914\n",
            "Epoch: 215, Loss: 0.880351334810257\n",
            "Epoch: 216, Loss: 0.8797622715172014\n",
            "Epoch: 217, Loss: 0.8791889830639488\n",
            "Epoch: 218, Loss: 0.8786306836103138\n",
            "Epoch: 219, Loss: 0.8780898404748816\n",
            "Epoch: 220, Loss: 0.8775545860591688\n",
            "Epoch: 221, Loss: 0.8770315960833901\n",
            "Epoch: 222, Loss: 0.8765118373067755\n",
            "Epoch: 223, Loss: 0.8760119002116354\n",
            "Epoch: 224, Loss: 0.8754945130724656\n",
            "Epoch: 225, Loss: 0.8749975483668478\n",
            "Epoch: 226, Loss: 0.8745089957588598\n",
            "Epoch: 227, Loss: 0.8740142207396658\n",
            "Epoch: 228, Loss: 0.8735435793274328\n",
            "Epoch: 229, Loss: 0.8730681522896415\n",
            "Epoch: 230, Loss: 0.8726022306241488\n",
            "Epoch: 231, Loss: 0.8721416655339693\n",
            "Epoch: 232, Loss: 0.8716874373586554\n",
            "Epoch: 233, Loss: 0.8712465637608579\n",
            "Epoch: 234, Loss: 0.8708050831368095\n",
            "Epoch: 235, Loss: 0.8703737917699312\n",
            "Epoch: 236, Loss: 0.8699359313437813\n",
            "Epoch: 237, Loss: 0.8695103526115417\n",
            "Epoch: 238, Loss: 0.8690888818941618\n",
            "Epoch: 239, Loss: 0.8686792066222743\n",
            "Epoch: 240, Loss: 0.868266141728351\n",
            "Epoch: 241, Loss: 0.8678556963017112\n",
            "Epoch: 242, Loss: 0.8674570651430833\n",
            "Epoch: 243, Loss: 0.8670428928576017\n",
            "Epoch: 244, Loss: 0.866641104221344\n",
            "Epoch: 245, Loss: 0.8662531673908234\n",
            "Epoch: 246, Loss: 0.8658474668076164\n",
            "Epoch: 247, Loss: 0.8654741299779791\n",
            "Epoch: 248, Loss: 0.8650832740884078\n",
            "Epoch: 249, Loss: 0.8647026451010453\n",
            "Epoch: 250, Loss: 0.8643355510736767\n",
            "Epoch: 251, Loss: 0.8639585375785828\n",
            "Epoch: 252, Loss: 0.8635861904997575\n",
            "Epoch: 253, Loss: 0.8632284998893738\n",
            "Epoch: 254, Loss: 0.8628627752002916\n",
            "Epoch: 255, Loss: 0.8625126088920393\n",
            "Epoch: 256, Loss: 0.8621544696782765\n",
            "Epoch: 257, Loss: 0.8618219526190507\n",
            "Epoch: 258, Loss: 0.8614660658334431\n",
            "Epoch: 259, Loss: 0.8611386230117396\n",
            "Epoch: 260, Loss: 0.8607935811343946\n",
            "Epoch: 261, Loss: 0.8604609809423748\n",
            "Epoch: 262, Loss: 0.8601402640342712\n",
            "Epoch: 263, Loss: 0.859816490035308\n",
            "Epoch: 264, Loss: 0.859495853122912\n",
            "Epoch: 265, Loss: 0.859170066682916\n",
            "Epoch: 266, Loss: 0.8588487066720661\n",
            "Epoch: 267, Loss: 0.8585236856811925\n",
            "Epoch: 268, Loss: 0.8582179797323126\n",
            "Epoch: 269, Loss: 0.8579042193136717\n",
            "Epoch: 270, Loss: 0.8575962436826605\n",
            "Epoch: 271, Loss: 0.8572897565992255\n",
            "Epoch: 272, Loss: 0.8569934211279217\n",
            "Epoch: 273, Loss: 0.8566823868375075\n",
            "Epoch: 274, Loss: 0.8563764409015053\n",
            "Epoch: 275, Loss: 0.8560942568277058\n",
            "Epoch: 276, Loss: 0.8557905241062767\n",
            "Epoch: 277, Loss: 0.8555097297618264\n",
            "Epoch: 278, Loss: 0.8552238439258776\n",
            "Epoch: 279, Loss: 0.854935032756705\n",
            "Epoch: 280, Loss: 0.8546530808273115\n",
            "Epoch: 281, Loss: 0.8543674851718702\n",
            "Epoch: 282, Loss: 0.8540920254431272\n",
            "Epoch: 283, Loss: 0.8538160261354948\n",
            "Epoch: 284, Loss: 0.8535452688995161\n",
            "Epoch: 285, Loss: 0.8532750073232149\n",
            "Epoch: 286, Loss: 0.8529925346374512\n",
            "Epoch: 287, Loss: 0.8527247372426485\n",
            "Epoch: 288, Loss: 0.8524622148589084\n",
            "Epoch: 289, Loss: 0.8521867425818193\n",
            "Epoch: 290, Loss: 0.8519302214446821\n",
            "Epoch: 291, Loss: 0.8516647894131509\n",
            "Epoch: 292, Loss: 0.851405016685787\n",
            "Epoch: 293, Loss: 0.8511486916165603\n",
            "Epoch: 294, Loss: 0.8508889847680142\n",
            "Epoch: 295, Loss: 0.8506322142324949\n",
            "Epoch: 296, Loss: 0.8503774530009219\n",
            "Epoch: 297, Loss: 0.8501267417481071\n",
            "Epoch: 298, Loss: 0.8498763347926893\n",
            "Epoch: 299, Loss: 0.8496221727446506\n",
            "Epoch: 300, Loss: 0.8493763333872745\n",
            "Epoch: 301, Loss: 0.849130580299779\n",
            "Epoch: 302, Loss: 0.848888845820176\n",
            "Epoch: 303, Loss: 0.8486480289383939\n",
            "Epoch: 304, Loss: 0.8484054348970714\n",
            "Epoch: 305, Loss: 0.8481659591197968\n",
            "Epoch: 306, Loss: 0.8479262433553997\n",
            "Epoch: 307, Loss: 0.8476888160956534\n",
            "Epoch: 308, Loss: 0.847442561074307\n",
            "Epoch: 309, Loss: 0.8472168445587158\n",
            "Epoch: 310, Loss: 0.8469848821037694\n",
            "Epoch: 311, Loss: 0.8467370459907934\n",
            "Epoch: 312, Loss: 0.8464957773685455\n",
            "Epoch: 313, Loss: 0.8462610448661604\n",
            "Epoch: 314, Loss: 0.8460242748260498\n",
            "Epoch: 315, Loss: 0.8458005001670436\n",
            "Epoch: 316, Loss: 0.8455645598863301\n",
            "Epoch: 317, Loss: 0.8453289364513598\n",
            "Epoch: 318, Loss: 0.8451062848693446\n",
            "Epoch: 319, Loss: 0.8448674035699744\n",
            "Epoch: 320, Loss: 0.8446433246135712\n",
            "Epoch: 321, Loss: 0.844413758892762\n",
            "Epoch: 322, Loss: 0.844152503891995\n",
            "Epoch: 323, Loss: 0.8439059775126608\n",
            "Epoch: 324, Loss: 0.8436625035185563\n",
            "Epoch: 325, Loss: 0.8434199941785712\n",
            "Epoch: 326, Loss: 0.8431660015332071\n",
            "Epoch: 327, Loss: 0.8429186218663266\n",
            "Epoch: 328, Loss: 0.8426610592164492\n",
            "Epoch: 329, Loss: 0.8424153955359208\n",
            "Epoch: 330, Loss: 0.8421700455640492\n",
            "Epoch: 331, Loss: 0.8419445172736519\n",
            "Epoch: 332, Loss: 0.8416853374556491\n",
            "Epoch: 333, Loss: 0.8414396878920103\n",
            "Epoch: 334, Loss: 0.8411805300336135\n",
            "Epoch: 335, Loss: 0.8409516074155506\n",
            "Epoch: 336, Loss: 0.8407051139756253\n",
            "Epoch: 337, Loss: 0.8404582017346433\n",
            "Epoch: 338, Loss: 0.8402120376888075\n",
            "Epoch: 339, Loss: 0.8399571478366852\n",
            "Epoch: 340, Loss: 0.8397054107565629\n",
            "Epoch: 341, Loss: 0.8394673654907628\n",
            "Epoch: 342, Loss: 0.8392365449353268\n",
            "Epoch: 343, Loss: 0.8390012257977536\n",
            "Epoch: 344, Loss: 0.8387867322093562\n",
            "Epoch: 345, Loss: 0.8385556167677829\n",
            "Epoch: 346, Loss: 0.838334183943899\n",
            "Epoch: 347, Loss: 0.8381068643770719\n",
            "Epoch: 348, Loss: 0.8378824136759105\n",
            "Epoch: 349, Loss: 0.8376502598586836\n",
            "Epoch: 350, Loss: 0.837419403226752\n",
            "Epoch: 351, Loss: 0.8372020344985159\n",
            "Epoch: 352, Loss: 0.8369676768779755\n",
            "Epoch: 353, Loss: 0.8367506955799303\n",
            "Epoch: 354, Loss: 0.8365278196962256\n",
            "Epoch: 355, Loss: 0.8363131708220432\n",
            "Epoch: 356, Loss: 0.8360915011481235\n",
            "Epoch: 357, Loss: 0.8358823923688186\n",
            "Epoch: 358, Loss: 0.8356638833096153\n",
            "Epoch: 359, Loss: 0.835457350078382\n",
            "Epoch: 360, Loss: 0.8352538206075367\n",
            "Epoch: 361, Loss: 0.8350403277497542\n",
            "Epoch: 362, Loss: 0.8348339043165508\n",
            "Epoch: 363, Loss: 0.8346327919709055\n",
            "Epoch: 364, Loss: 0.8344300593200483\n",
            "Epoch: 365, Loss: 0.8342411847491014\n",
            "Epoch: 366, Loss: 0.8340398292792471\n",
            "Epoch: 367, Loss: 0.8338376989490107\n",
            "Epoch: 368, Loss: 0.8336399840681177\n",
            "Epoch: 369, Loss: 0.833428492671565\n",
            "Epoch: 370, Loss: 0.8332330333559137\n",
            "Epoch: 371, Loss: 0.8330410452265489\n",
            "Epoch: 372, Loss: 0.8328358813336021\n",
            "Epoch: 373, Loss: 0.832640119289097\n",
            "Epoch: 374, Loss: 0.8324461391097621\n",
            "Epoch: 375, Loss: 0.8322510562444988\n",
            "Epoch: 376, Loss: 0.8320726062122145\n",
            "Epoch: 377, Loss: 0.8318776064797452\n",
            "Epoch: 378, Loss: 0.8316796327892103\n",
            "Epoch: 379, Loss: 0.8314854173283828\n",
            "Epoch: 380, Loss: 0.8312842375353763\n",
            "Epoch: 381, Loss: 0.8310975651991995\n",
            "Epoch: 382, Loss: 0.83089626933399\n",
            "Epoch: 383, Loss: 0.8307093585792341\n",
            "Epoch: 384, Loss: 0.8305198707078633\n",
            "Epoch: 385, Loss: 0.8303263234464746\n",
            "Epoch: 386, Loss: 0.8301316499710083\n",
            "Epoch: 387, Loss: 0.8299454905484852\n",
            "Epoch: 388, Loss: 0.8297432237549832\n",
            "Epoch: 389, Loss: 0.8295440720884424\n",
            "Epoch: 390, Loss: 0.829345460000791\n",
            "Epoch: 391, Loss: 0.829131145226328\n",
            "Epoch: 392, Loss: 0.828922091346038\n",
            "Epoch: 393, Loss: 0.8287177242730793\n",
            "Epoch: 394, Loss: 0.8285162072432669\n",
            "Epoch: 395, Loss: 0.8283129993237948\n",
            "Epoch: 396, Loss: 0.8281044928651107\n",
            "Epoch: 397, Loss: 0.827922240683907\n",
            "Epoch: 398, Loss: 0.8277108700651872\n",
            "Epoch: 399, Loss: 0.8275147033365149\n",
            "Epoch: 400, Loss: 0.8273010222535384\n",
            "Epoch: 401, Loss: 0.8271083329853258\n",
            "Epoch: 402, Loss: 0.8269069775154716\n",
            "Epoch: 403, Loss: 0.8267163697041964\n",
            "Epoch: 404, Loss: 0.82652148604393\n",
            "Epoch: 405, Loss: 0.8263336435744637\n",
            "Epoch: 406, Loss: 0.8261400696478391\n",
            "Epoch: 407, Loss: 0.8259360162835372\n",
            "Epoch: 408, Loss: 0.8257571772525185\n",
            "Epoch: 409, Loss: 0.8255637429262462\n",
            "Epoch: 410, Loss: 0.8253849650684156\n",
            "Epoch: 411, Loss: 0.8251979586325193\n",
            "Epoch: 412, Loss: 0.8250009072454352\n",
            "Epoch: 413, Loss: 0.8248228932681837\n",
            "Epoch: 414, Loss: 0.824631846264789\n",
            "Epoch: 415, Loss: 0.824447689872039\n",
            "Epoch: 416, Loss: 0.8242667835009726\n",
            "Epoch: 417, Loss: 0.8240799888184196\n",
            "Epoch: 418, Loss: 0.8238935423524756\n",
            "Epoch: 419, Loss: 0.8237157040520718\n",
            "Epoch: 420, Loss: 0.8235301281276503\n",
            "Epoch: 421, Loss: 0.8233460344766316\n",
            "Epoch: 422, Loss: 0.8231620098415174\n",
            "Epoch: 423, Loss: 0.822976678609848\n",
            "Epoch: 424, Loss: 0.822801505264483\n",
            "Epoch: 425, Loss: 0.8226227227010225\n",
            "Epoch: 426, Loss: 0.8224289213356218\n",
            "Epoch: 427, Loss: 0.822241474139063\n",
            "Epoch: 428, Loss: 0.8220625347212741\n",
            "Epoch: 429, Loss: 0.8218779956039629\n",
            "Epoch: 430, Loss: 0.8216900041228846\n",
            "Epoch: 431, Loss: 0.8215133045849047\n",
            "Epoch: 432, Loss: 0.8213235218273965\n",
            "Epoch: 433, Loss: 0.8211513757705688\n",
            "Epoch: 434, Loss: 0.8209738182394128\n",
            "Epoch: 435, Loss: 0.8207875098052778\n",
            "Epoch: 436, Loss: 0.8206168695500022\n",
            "Epoch: 437, Loss: 0.8204272938402075\n",
            "Epoch: 438, Loss: 0.8202491308513441\n",
            "Epoch: 439, Loss: 0.820073946526176\n",
            "Epoch: 440, Loss: 0.8198841371034321\n",
            "Epoch: 441, Loss: 0.8197052729757208\n",
            "Epoch: 442, Loss: 0.8195213816667858\n",
            "Epoch: 443, Loss: 0.8193395137786865\n",
            "Epoch: 444, Loss: 0.8191525261653098\n",
            "Epoch: 445, Loss: 0.8189794530994013\n",
            "Epoch: 446, Loss: 0.8188034986194811\n",
            "Epoch: 447, Loss: 0.8186123449551431\n",
            "Epoch: 448, Loss: 0.8184067104992113\n",
            "Epoch: 449, Loss: 0.8182075760866466\n",
            "Epoch: 450, Loss: 0.8180193540297056\n",
            "Epoch: 451, Loss: 0.8178275603997079\n",
            "Epoch: 452, Loss: 0.8176386905343909\n",
            "Epoch: 453, Loss: 0.8174584680481961\n",
            "Epoch: 454, Loss: 0.8172566969143716\n",
            "Epoch: 455, Loss: 0.8170751348922127\n",
            "Epoch: 456, Loss: 0.8168856454522986\n",
            "Epoch: 457, Loss: 0.8167003973534233\n",
            "Epoch: 458, Loss: 0.8165144559584165\n",
            "Epoch: 459, Loss: 0.8163329786375949\n",
            "Epoch: 460, Loss: 0.8161582131134836\n",
            "Epoch: 461, Loss: 0.8159735892948351\n",
            "Epoch: 462, Loss: 0.8157841390685031\n",
            "Epoch: 463, Loss: 0.8156037330627441\n",
            "Epoch: 464, Loss: 0.8154127958573794\n",
            "Epoch: 465, Loss: 0.8152277093184622\n",
            "Epoch: 466, Loss: 0.8150422729943928\n",
            "Epoch: 467, Loss: 0.8148660848015233\n",
            "Epoch: 468, Loss: 0.8146798485203793\n",
            "Epoch: 469, Loss: 0.8144986943194741\n",
            "Epoch: 470, Loss: 0.8143246597365329\n",
            "Epoch: 471, Loss: 0.8141352957800815\n",
            "Epoch: 472, Loss: 0.813958161755612\n",
            "Epoch: 473, Loss: 0.8137780067167784\n",
            "Epoch: 474, Loss: 0.8136048928687447\n",
            "Epoch: 475, Loss: 0.8134080281383113\n",
            "Epoch: 476, Loss: 0.8132166140957883\n",
            "Epoch: 477, Loss: 0.8130354097014979\n",
            "Epoch: 478, Loss: 0.8128399660712794\n",
            "Epoch: 479, Loss: 0.812663313589598\n",
            "Epoch: 480, Loss: 0.8124860964323345\n",
            "Epoch: 481, Loss: 0.8122887360422235\n",
            "Epoch: 482, Loss: 0.8121120082704645\n",
            "Epoch: 483, Loss: 0.811942530305762\n",
            "Epoch: 484, Loss: 0.8117550470327076\n",
            "Epoch: 485, Loss: 0.8115751084528471\n",
            "Epoch: 486, Loss: 0.8113993657262701\n",
            "Epoch: 487, Loss: 0.8112124518344277\n",
            "Epoch: 488, Loss: 0.8110342041442269\n",
            "Epoch: 489, Loss: 0.8108555486327723\n",
            "Epoch: 490, Loss: 0.8106772507491865\n",
            "Epoch: 491, Loss: 0.8104879479659232\n",
            "Epoch: 492, Loss: 0.8103104029831133\n",
            "Epoch: 493, Loss: 0.8101364578071394\n",
            "Epoch: 494, Loss: 0.809956254143464\n",
            "Epoch: 495, Loss: 0.809776706130881\n",
            "Epoch: 496, Loss: 0.8096161531774622\n",
            "Epoch: 497, Loss: 0.8094575044355894\n",
            "Epoch: 498, Loss: 0.8092672903286783\n",
            "Epoch: 499, Loss: 0.8090851651994806\n",
            "Epoch: 500, Loss: 0.8089140920262587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SWrtvI_-Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = []\n",
        "for i, (X, y) in enumerate(test_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    X = X.to(device)\n",
        "    output = model(X)   \n",
        "    test_outputs.append(output.cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJkxyJMIAHbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs_converted = []\n",
        "for i in range(len(test_outputs)):\n",
        "  current = test_outputs[i][0]\n",
        "  add = [1 if x >= 0.50 else 0 for x in current]\n",
        "  test_outputs_converted.append(add)\n",
        "test_outputs_converted = np.array(test_outputs_converted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SKd8iZbAItN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a0793a8-68a6-47e3-a9d0-a4fb5d6f0122"
      },
      "source": [
        "accuracy = 0\n",
        "missed = []\n",
        "for i in range(len(test_outputs_converted)):\n",
        "  if collections.Counter(test_outputs_converted[i]) == collections.Counter(converted_test[i]):\n",
        "    accuracy += 1\n",
        "  else:\n",
        "    missed.append(i)\n",
        "print(f'Testing Accuracy: {accuracy/33}')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.7272727272727273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw0fU_g-4cQ1",
        "colab_type": "text"
      },
      "source": [
        "# Factor Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3_PEnCY5Qze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import FactorAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGfYgxbN5iR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "factors = FactorAnalysis(n_components = 100, copy = True, random_state = 15)\n",
        "fa_train_genes = factors.fit_transform(train_data)\n",
        "fa_test_genes = factors.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wI8ar7o5oEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_weights = pd.DataFrame(factors.components_)\n",
        "col_names = train_data.columns.values\n",
        "all_weights.columns = col_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbKCCJ8x5ttV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fa_important_genes = []\n",
        "temp = pd.DataFrame(np.abs(all_weights)).T\n",
        "for i in range(50):\n",
        "    fa_important_genes.extend(temp.nlargest(100, i).index.values)\n",
        "fa_important_genes = np.unique(fa_important_genes)\n",
        "fa_important_train = train_data[fa_important_genes]\n",
        "fa_important_test = test_data[fa_important_genes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voHtXJC95zCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21c408c2-1f44-426e-bec0-da5628164486"
      },
      "source": [
        "c = m.LogisticIT()\n",
        "cv_strategy = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
        "param_grid = {'alpha': np.logspace(-3,3,7)}\n",
        "ridge_grid_fa = GridSearchCV(c, param_grid, cv=cv_strategy, n_jobs=-1, return_train_score=True)\n",
        "ridge_grid_fa.fit(np.array(fa_important_train), np.array(mapped_labels_train))\n",
        "ridge_grid_fa.best_score_"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6854022988505747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IsUt_1X7kBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67af82ac-7074-45ab-fef7-8b567d186269"
      },
      "source": [
        "ridge_grid_fa.score(np.array(fa_important_test), np.array(mapped_labels_test))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272727272727273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMlcyVsOSz-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi = permutation_importance(ridge_grid_fa.best_estimator_, fa_important_train, mapped_labels_train, n_repeats=10, random_state=0)\n",
        "pi_mean_vals = pi['importances_mean']\n",
        "cols = pd.DataFrame(fa_important_train).columns\n",
        "pi_importances_pairs = zip(cols, pi_mean_vals)\n",
        "pi_pairs_sorted = sorted(pi_importances_pairs, key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o85yC4PhTYC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_zero_fa = [int(x[0]) for x in pi_pairs_sorted if x[1] > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N034qDzcTgdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selector = RFE(ridge_grid_fa.best_estimator_, 1, step=1)\n",
        "selector = selector.fit(fa_important_train, mapped_labels_train)\n",
        "ranking_pairs = zip(pd.DataFrame(fa_important_train).columns, selector.ranking_)\n",
        "ranking_pairs_sorted = sorted(ranking_pairs, key=lambda x: x[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEAqG9eaT6tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_100_rfe_fa = []\n",
        "for i in range(100):\n",
        "  top_100_rfe_fa.append(int(ranking_pairs_sorted[i][0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itrbg5rDAzqa",
        "colab_type": "text"
      },
      "source": [
        "# Retrain Neural Network using FA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOme6nmaA3pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tensor = torch.tensor(fa_important_train.values.astype(np.float32)) \n",
        "test_tensor = torch.tensor(fa_important_test.values.astype(np.float32))\n",
        "batch_size = 16\n",
        "# Define datasets and dataloaders\n",
        "testing_dataset = TensorDataset(test_tensor, test_target_tensor)\n",
        "training_dataset = TensorDataset(train_tensor, train_target_tensor)\n",
        "test_dataloader = DataLoader(testing_dataset, batch_size = 1)\n",
        "train_dataloader = DataLoader(training_dataset, batch_size = batch_size )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj3Mh_3dBCcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class rnn_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(rnn_model, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(len(fa_important_train.columns), 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 3),\n",
        "            # Sigmoid funtion on each of the output nodes\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # The last layer hidden states from BERT are used as embeddings/semantic \n",
        "    # representation for the input and therefore, passed into the MLP\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrqFJZABIZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile and train model\n",
        "model = rnn_model()\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmy_tgttBL6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4585c696-475b-414d-c37a-f1981502c596"
      },
      "source": [
        "epochs = 500\n",
        "for e in range(0, epochs):\n",
        "    model.train()\n",
        "    current_loss = 0\n",
        "    for i, (X, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # To GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device).double()\n",
        "        output = model(X).double()\n",
        "        loss = loss_function(output, y)\n",
        "        loss = loss.type(torch.cuda.FloatTensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f'Epoch: {e+1}, Loss: {current_loss/len(train_dataloader)}')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 2.125166052266171\n",
            "Epoch: 2, Loss: 2.007887695965014\n",
            "Epoch: 3, Loss: 1.9084436830721403\n",
            "Epoch: 4, Loss: 1.814291194865578\n",
            "Epoch: 5, Loss: 1.7224170659717761\n",
            "Epoch: 6, Loss: 1.6308397682089555\n",
            "Epoch: 7, Loss: 1.5438754746788426\n",
            "Epoch: 8, Loss: 1.4668110483571102\n",
            "Epoch: 9, Loss: 1.4009725294615094\n",
            "Epoch: 10, Loss: 1.3448578558470075\n",
            "Epoch: 11, Loss: 1.2968745106144954\n",
            "Epoch: 12, Loss: 1.2551636758603548\n",
            "Epoch: 13, Loss: 1.2179283091896458\n",
            "Epoch: 14, Loss: 1.1844127178192139\n",
            "Epoch: 15, Loss: 1.1540375728356211\n",
            "Epoch: 16, Loss: 1.1260926378400702\n",
            "Epoch: 17, Loss: 1.1002675451730426\n",
            "Epoch: 18, Loss: 1.0763151300580878\n",
            "Epoch: 19, Loss: 1.0541409128590633\n",
            "Epoch: 20, Loss: 1.0337836961997182\n",
            "Epoch: 21, Loss: 1.0149140546196385\n",
            "Epoch: 22, Loss: 0.9974033832550049\n",
            "Epoch: 23, Loss: 0.9810020233455458\n",
            "Epoch: 24, Loss: 0.9656160567936144\n",
            "Epoch: 25, Loss: 0.9512904631464105\n",
            "Epoch: 26, Loss: 0.93794668348212\n",
            "Epoch: 27, Loss: 0.9254364590895804\n",
            "Epoch: 28, Loss: 0.9136322987707037\n",
            "Epoch: 29, Loss: 0.9025025493220279\n",
            "Epoch: 30, Loss: 0.8919933438301086\n",
            "Epoch: 31, Loss: 0.8820547618364033\n",
            "Epoch: 32, Loss: 0.872597452841307\n",
            "Epoch: 33, Loss: 0.8635443103940863\n",
            "Epoch: 34, Loss: 0.8548615668949328\n",
            "Epoch: 35, Loss: 0.8463903257721349\n",
            "Epoch: 36, Loss: 0.8383103891422874\n",
            "Epoch: 37, Loss: 0.8305457799058211\n",
            "Epoch: 38, Loss: 0.8231873104446813\n",
            "Epoch: 39, Loss: 0.8160698382478011\n",
            "Epoch: 40, Loss: 0.8093867960729098\n",
            "Epoch: 41, Loss: 0.803050508624629\n",
            "Epoch: 42, Loss: 0.7969717069676048\n",
            "Epoch: 43, Loss: 0.7912355112402063\n",
            "Epoch: 44, Loss: 0.785705389160859\n",
            "Epoch: 45, Loss: 0.7804131084366849\n",
            "Epoch: 46, Loss: 0.7752274714018169\n",
            "Epoch: 47, Loss: 0.7701407812143627\n",
            "Epoch: 48, Loss: 0.7652328688847391\n",
            "Epoch: 49, Loss: 0.7605097999698237\n",
            "Epoch: 50, Loss: 0.7558753584560595\n",
            "Epoch: 51, Loss: 0.7514088953796186\n",
            "Epoch: 52, Loss: 0.7470816138543581\n",
            "Epoch: 53, Loss: 0.7428438224290547\n",
            "Epoch: 54, Loss: 0.7387220718358692\n",
            "Epoch: 55, Loss: 0.7346900842691723\n",
            "Epoch: 56, Loss: 0.7307712671003843\n",
            "Epoch: 57, Loss: 0.7268804327437752\n",
            "Epoch: 58, Loss: 0.723064295555416\n",
            "Epoch: 59, Loss: 0.7192836639128233\n",
            "Epoch: 60, Loss: 0.7156121919029638\n",
            "Epoch: 61, Loss: 0.7119607219570562\n",
            "Epoch: 62, Loss: 0.708378835728294\n",
            "Epoch: 63, Loss: 0.7048642462805698\n",
            "Epoch: 64, Loss: 0.7013828017209706\n",
            "Epoch: 65, Loss: 0.6979041868134549\n",
            "Epoch: 66, Loss: 0.6944884046127922\n",
            "Epoch: 67, Loss: 0.691078223680195\n",
            "Epoch: 68, Loss: 0.6876405712805296\n",
            "Epoch: 69, Loss: 0.6842455722783741\n",
            "Epoch: 70, Loss: 0.6809743736919603\n",
            "Epoch: 71, Loss: 0.6776109886796851\n",
            "Epoch: 72, Loss: 0.6743760704994202\n",
            "Epoch: 73, Loss: 0.6711075164769825\n",
            "Epoch: 74, Loss: 0.6679931345738863\n",
            "Epoch: 75, Loss: 0.664917318444503\n",
            "Epoch: 76, Loss: 0.6617787747006667\n",
            "Epoch: 77, Loss: 0.6587401455954501\n",
            "Epoch: 78, Loss: 0.6557148287170812\n",
            "Epoch: 79, Loss: 0.6527369257650877\n",
            "Epoch: 80, Loss: 0.6497571609522167\n",
            "Epoch: 81, Loss: 0.6467959457322171\n",
            "Epoch: 82, Loss: 0.6438946865106884\n",
            "Epoch: 83, Loss: 0.6408641228550359\n",
            "Epoch: 84, Loss: 0.6379857267204084\n",
            "Epoch: 85, Loss: 0.6349808165901586\n",
            "Epoch: 86, Loss: 0.6321188525149697\n",
            "Epoch: 87, Loss: 0.629126572295239\n",
            "Epoch: 88, Loss: 0.6262095021574121\n",
            "Epoch: 89, Loss: 0.6233567877819663\n",
            "Epoch: 90, Loss: 0.6204311675147006\n",
            "Epoch: 91, Loss: 0.6175213409097571\n",
            "Epoch: 92, Loss: 0.6145251314891013\n",
            "Epoch: 93, Loss: 0.6113350093364716\n",
            "Epoch: 94, Loss: 0.6081136998377348\n",
            "Epoch: 95, Loss: 0.604654062735407\n",
            "Epoch: 96, Loss: 0.6010392198437139\n",
            "Epoch: 97, Loss: 0.5975618048718101\n",
            "Epoch: 98, Loss: 0.5942061276812303\n",
            "Epoch: 99, Loss: 0.5908588861164293\n",
            "Epoch: 100, Loss: 0.5875630582633772\n",
            "Epoch: 101, Loss: 0.584253662510922\n",
            "Epoch: 102, Loss: 0.5808345157849161\n",
            "Epoch: 103, Loss: 0.5774083843356684\n",
            "Epoch: 104, Loss: 0.5742061169523942\n",
            "Epoch: 105, Loss: 0.5710359780411971\n",
            "Epoch: 106, Loss: 0.5677073880245811\n",
            "Epoch: 107, Loss: 0.5646645505177347\n",
            "Epoch: 108, Loss: 0.5613882337745867\n",
            "Epoch: 109, Loss: 0.5584389379149989\n",
            "Epoch: 110, Loss: 0.5553093834927207\n",
            "Epoch: 111, Loss: 0.5520842200831363\n",
            "Epoch: 112, Loss: 0.5489370022949419\n",
            "Epoch: 113, Loss: 0.5458992782392\n",
            "Epoch: 114, Loss: 0.5427676188318353\n",
            "Epoch: 115, Loss: 0.5397488098395499\n",
            "Epoch: 116, Loss: 0.536697506904602\n",
            "Epoch: 117, Loss: 0.5336025121964907\n",
            "Epoch: 118, Loss: 0.5304732118782244\n",
            "Epoch: 119, Loss: 0.5275539112718481\n",
            "Epoch: 120, Loss: 0.524489562762411\n",
            "Epoch: 121, Loss: 0.5215357949859217\n",
            "Epoch: 122, Loss: 0.5185404890461972\n",
            "Epoch: 123, Loss: 0.515632257649773\n",
            "Epoch: 124, Loss: 0.5127156706232774\n",
            "Epoch: 125, Loss: 0.5097631278790926\n",
            "Epoch: 126, Loss: 0.5069734312986073\n",
            "Epoch: 127, Loss: 0.5041547731349343\n",
            "Epoch: 128, Loss: 0.5013123311494526\n",
            "Epoch: 129, Loss: 0.4984854334279111\n",
            "Epoch: 130, Loss: 0.49564478350313085\n",
            "Epoch: 131, Loss: 0.4928826830889049\n",
            "Epoch: 132, Loss: 0.4901151084586194\n",
            "Epoch: 133, Loss: 0.4873762585614857\n",
            "Epoch: 134, Loss: 0.4845805717142005\n",
            "Epoch: 135, Loss: 0.48190264168538544\n",
            "Epoch: 136, Loss: 0.47916946756212336\n",
            "Epoch: 137, Loss: 0.47652338758895274\n",
            "Epoch: 138, Loss: 0.4736690293801458\n",
            "Epoch: 139, Loss: 0.470970891023937\n",
            "Epoch: 140, Loss: 0.4682490135494031\n",
            "Epoch: 141, Loss: 0.4654901655096757\n",
            "Epoch: 142, Loss: 0.4626889228820801\n",
            "Epoch: 143, Loss: 0.4600391882030587\n",
            "Epoch: 144, Loss: 0.457238550248899\n",
            "Epoch: 145, Loss: 0.4546574793363872\n",
            "Epoch: 146, Loss: 0.45184601215939774\n",
            "Epoch: 147, Loss: 0.4493163094708794\n",
            "Epoch: 148, Loss: 0.44658234637034566\n",
            "Epoch: 149, Loss: 0.4440106404455085\n",
            "Epoch: 150, Loss: 0.4413236709017503\n",
            "Epoch: 151, Loss: 0.4386049548262044\n",
            "Epoch: 152, Loss: 0.4360399105046925\n",
            "Epoch: 153, Loss: 0.433265031952607\n",
            "Epoch: 154, Loss: 0.43067791979563863\n",
            "Epoch: 155, Loss: 0.427957263432051\n",
            "Epoch: 156, Loss: 0.4254161182202791\n",
            "Epoch: 157, Loss: 0.42280105932762746\n",
            "Epoch: 158, Loss: 0.4201946140904176\n",
            "Epoch: 159, Loss: 0.4175671056697243\n",
            "Epoch: 160, Loss: 0.41503971422973435\n",
            "Epoch: 161, Loss: 0.4123630288400148\n",
            "Epoch: 162, Loss: 0.4098278417399055\n",
            "Epoch: 163, Loss: 0.40727847343996953\n",
            "Epoch: 164, Loss: 0.404694590913622\n",
            "Epoch: 165, Loss: 0.4020944739642896\n",
            "Epoch: 166, Loss: 0.3996102629523528\n",
            "Epoch: 167, Loss: 0.3969483799056003\n",
            "Epoch: 168, Loss: 0.3944168922148253\n",
            "Epoch: 169, Loss: 0.3919239804932946\n",
            "Epoch: 170, Loss: 0.38936056745679753\n",
            "Epoch: 171, Loss: 0.38686160351100723\n",
            "Epoch: 172, Loss: 0.3843831286618584\n",
            "Epoch: 173, Loss: 0.3818001245197497\n",
            "Epoch: 174, Loss: 0.3794204969155161\n",
            "Epoch: 175, Loss: 0.3768717317204726\n",
            "Epoch: 176, Loss: 0.3744924225305256\n",
            "Epoch: 177, Loss: 0.3719037982978319\n",
            "Epoch: 178, Loss: 0.3694731514704855\n",
            "Epoch: 179, Loss: 0.36702641923176615\n",
            "Epoch: 180, Loss: 0.3646557629108429\n",
            "Epoch: 181, Loss: 0.3620628243998477\n",
            "Epoch: 182, Loss: 0.35977429935806676\n",
            "Epoch: 183, Loss: 0.3572907322331479\n",
            "Epoch: 184, Loss: 0.3548598642411985\n",
            "Epoch: 185, Loss: 0.3524543211648339\n",
            "Epoch: 186, Loss: 0.35005449542873784\n",
            "Epoch: 187, Loss: 0.34771984658743205\n",
            "Epoch: 188, Loss: 0.3453035040905601\n",
            "Epoch: 189, Loss: 0.3430389856037341\n",
            "Epoch: 190, Loss: 0.3405716168253045\n",
            "Epoch: 191, Loss: 0.3383350952675468\n",
            "Epoch: 192, Loss: 0.33605120291835383\n",
            "Epoch: 193, Loss: 0.33364759385585785\n",
            "Epoch: 194, Loss: 0.3314590658012189\n",
            "Epoch: 195, Loss: 0.32917134855922897\n",
            "Epoch: 196, Loss: 0.32692919357826833\n",
            "Epoch: 197, Loss: 0.3246127382705086\n",
            "Epoch: 198, Loss: 0.32240601040815053\n",
            "Epoch: 199, Loss: 0.32013890304063497\n",
            "Epoch: 200, Loss: 0.31782333239128713\n",
            "Epoch: 201, Loss: 0.3156549318840629\n",
            "Epoch: 202, Loss: 0.3133779372039594\n",
            "Epoch: 203, Loss: 0.311112788947005\n",
            "Epoch: 204, Loss: 0.3089427030400226\n",
            "Epoch: 205, Loss: 0.30679006952988475\n",
            "Epoch: 206, Loss: 0.30455564589876877\n",
            "Epoch: 207, Loss: 0.302451911725496\n",
            "Epoch: 208, Loss: 0.3002359969051261\n",
            "Epoch: 209, Loss: 0.2981752517976259\n",
            "Epoch: 210, Loss: 0.2959907486250526\n",
            "Epoch: 211, Loss: 0.29382704825777756\n",
            "Epoch: 212, Loss: 0.29175470612551035\n",
            "Epoch: 213, Loss: 0.2896067006023307\n",
            "Epoch: 214, Loss: 0.28754680564529017\n",
            "Epoch: 215, Loss: 0.28542372113779974\n",
            "Epoch: 216, Loss: 0.28337415582255315\n",
            "Epoch: 217, Loss: 0.28128507654917867\n",
            "Epoch: 218, Loss: 0.27925806845489304\n",
            "Epoch: 219, Loss: 0.27719565049598094\n",
            "Epoch: 220, Loss: 0.2751894342271905\n",
            "Epoch: 221, Loss: 0.2731712640900361\n",
            "Epoch: 222, Loss: 0.27108447018422577\n",
            "Epoch: 223, Loss: 0.2690911857705367\n",
            "Epoch: 224, Loss: 0.26710959837624904\n",
            "Epoch: 225, Loss: 0.26520503077067825\n",
            "Epoch: 226, Loss: 0.263114118654477\n",
            "Epoch: 227, Loss: 0.2612124559910674\n",
            "Epoch: 228, Loss: 0.2592229251014559\n",
            "Epoch: 229, Loss: 0.2573533764010982\n",
            "Epoch: 230, Loss: 0.2553403016768004\n",
            "Epoch: 231, Loss: 0.2534501999616623\n",
            "Epoch: 232, Loss: 0.25156798449001816\n",
            "Epoch: 233, Loss: 0.24967747573789797\n",
            "Epoch: 234, Loss: 0.24774065221610822\n",
            "Epoch: 235, Loss: 0.2458413651115016\n",
            "Epoch: 236, Loss: 0.24403638627968335\n",
            "Epoch: 237, Loss: 0.24212379047745153\n",
            "Epoch: 238, Loss: 0.24030350148677826\n",
            "Epoch: 239, Loss: 0.23842955498318924\n",
            "Epoch: 240, Loss: 0.23669280543139107\n",
            "Epoch: 241, Loss: 0.23481203615665436\n",
            "Epoch: 242, Loss: 0.23304018770393573\n",
            "Epoch: 243, Loss: 0.2312221334953057\n",
            "Epoch: 244, Loss: 0.22945634158034073\n",
            "Epoch: 245, Loss: 0.22771589026639336\n",
            "Epoch: 246, Loss: 0.22597739610232806\n",
            "Epoch: 247, Loss: 0.22412598211514323\n",
            "Epoch: 248, Loss: 0.22252006279794792\n",
            "Epoch: 249, Loss: 0.2207563103813874\n",
            "Epoch: 250, Loss: 0.21907141177277817\n",
            "Epoch: 251, Loss: 0.21732695047792636\n",
            "Epoch: 252, Loss: 0.21562542883973373\n",
            "Epoch: 253, Loss: 0.2139451746877871\n",
            "Epoch: 254, Loss: 0.21226305553787633\n",
            "Epoch: 255, Loss: 0.2106026756136041\n",
            "Epoch: 256, Loss: 0.2088651182620149\n",
            "Epoch: 257, Loss: 0.2072112415181963\n",
            "Epoch: 258, Loss: 0.20549975374811574\n",
            "Epoch: 259, Loss: 0.20388986287932648\n",
            "Epoch: 260, Loss: 0.20225434515037036\n",
            "Epoch: 261, Loss: 0.20062316248291417\n",
            "Epoch: 262, Loss: 0.19901300770671745\n",
            "Epoch: 263, Loss: 0.197356137790178\n",
            "Epoch: 264, Loss: 0.19578352452893005\n",
            "Epoch: 265, Loss: 0.19414668138089933\n",
            "Epoch: 266, Loss: 0.19265603783883548\n",
            "Epoch: 267, Loss: 0.1910495973731342\n",
            "Epoch: 268, Loss: 0.1894836782624847\n",
            "Epoch: 269, Loss: 0.18795527321727654\n",
            "Epoch: 270, Loss: 0.1864212433758535\n",
            "Epoch: 271, Loss: 0.18486710088817696\n",
            "Epoch: 272, Loss: 0.18337692124278923\n",
            "Epoch: 273, Loss: 0.18180939790449643\n",
            "Epoch: 274, Loss: 0.18033514485547417\n",
            "Epoch: 275, Loss: 0.1788789269171263\n",
            "Epoch: 276, Loss: 0.17736347451021797\n",
            "Epoch: 277, Loss: 0.17590255878473582\n",
            "Epoch: 278, Loss: 0.17444872228722824\n",
            "Epoch: 279, Loss: 0.17298628546689687\n",
            "Epoch: 280, Loss: 0.1715863896043677\n",
            "Epoch: 281, Loss: 0.1701369360089302\n",
            "Epoch: 282, Loss: 0.1686993135433448\n",
            "Epoch: 283, Loss: 0.16732068712774076\n",
            "Epoch: 284, Loss: 0.1659399376888024\n",
            "Epoch: 285, Loss: 0.1645542912577328\n",
            "Epoch: 286, Loss: 0.1631670405990199\n",
            "Epoch: 287, Loss: 0.16179413936640086\n",
            "Epoch: 288, Loss: 0.16047045333605064\n",
            "Epoch: 289, Loss: 0.1591015858085532\n",
            "Epoch: 290, Loss: 0.15774265617916458\n",
            "Epoch: 291, Loss: 0.15645589071669078\n",
            "Epoch: 292, Loss: 0.15519799409728302\n",
            "Epoch: 293, Loss: 0.15382765625652514\n",
            "Epoch: 294, Loss: 0.15252610394044927\n",
            "Epoch: 295, Loss: 0.15127549634168022\n",
            "Epoch: 296, Loss: 0.1500499928860288\n",
            "Epoch: 297, Loss: 0.14875819259568265\n",
            "Epoch: 298, Loss: 0.14748262300303108\n",
            "Epoch: 299, Loss: 0.1462711366383653\n",
            "Epoch: 300, Loss: 0.1450144446601993\n",
            "Epoch: 301, Loss: 0.14384129114056887\n",
            "Epoch: 302, Loss: 0.14262017490048157\n",
            "Epoch: 303, Loss: 0.14132839206017947\n",
            "Epoch: 304, Loss: 0.14018628020819865\n",
            "Epoch: 305, Loss: 0.13895601524334206\n",
            "Epoch: 306, Loss: 0.13777051101389684\n",
            "Epoch: 307, Loss: 0.13664355932881958\n",
            "Epoch: 308, Loss: 0.13541229402548388\n",
            "Epoch: 309, Loss: 0.13426777602810608\n",
            "Epoch: 310, Loss: 0.13314621150493622\n",
            "Epoch: 311, Loss: 0.1319913652382399\n",
            "Epoch: 312, Loss: 0.13084967767721728\n",
            "Epoch: 313, Loss: 0.1297586026944612\n",
            "Epoch: 314, Loss: 0.12863672250195554\n",
            "Epoch: 315, Loss: 0.12748195172140472\n",
            "Epoch: 316, Loss: 0.12644352116867116\n",
            "Epoch: 317, Loss: 0.1253259778022766\n",
            "Epoch: 318, Loss: 0.12426791496967014\n",
            "Epoch: 319, Loss: 0.12314518246995776\n",
            "Epoch: 320, Loss: 0.12211211829593308\n",
            "Epoch: 321, Loss: 0.12101898107089494\n",
            "Epoch: 322, Loss: 0.12006889989501551\n",
            "Epoch: 323, Loss: 0.1189514341715135\n",
            "Epoch: 324, Loss: 0.11789329330387868\n",
            "Epoch: 325, Loss: 0.11691147422319964\n",
            "Epoch: 326, Loss: 0.11587470318926008\n",
            "Epoch: 327, Loss: 0.11486310276545976\n",
            "Epoch: 328, Loss: 0.11386435694600407\n",
            "Epoch: 329, Loss: 0.11286026925632828\n",
            "Epoch: 330, Loss: 0.11189328683050055\n",
            "Epoch: 331, Loss: 0.11089058436061207\n",
            "Epoch: 332, Loss: 0.10993141406460812\n",
            "Epoch: 333, Loss: 0.10900258135638739\n",
            "Epoch: 334, Loss: 0.10797102004289627\n",
            "Epoch: 335, Loss: 0.10707764621627958\n",
            "Epoch: 336, Loss: 0.10614146664738655\n",
            "Epoch: 337, Loss: 0.10517293999069616\n",
            "Epoch: 338, Loss: 0.10424832940885895\n",
            "Epoch: 339, Loss: 0.10332863464167244\n",
            "Epoch: 340, Loss: 0.10244647707594068\n",
            "Epoch: 341, Loss: 0.10150422842094772\n",
            "Epoch: 342, Loss: 0.10065643097224988\n",
            "Epoch: 343, Loss: 0.09971054564965398\n",
            "Epoch: 344, Loss: 0.09887704410051044\n",
            "Epoch: 345, Loss: 0.09794717399697554\n",
            "Epoch: 346, Loss: 0.09709191714462481\n",
            "Epoch: 347, Loss: 0.09623260756856516\n",
            "Epoch: 348, Loss: 0.09538467934257105\n",
            "Epoch: 349, Loss: 0.09448778923404844\n",
            "Epoch: 350, Loss: 0.09370153358108119\n",
            "Epoch: 351, Loss: 0.09284274417318795\n",
            "Epoch: 352, Loss: 0.09202054103738383\n",
            "Epoch: 353, Loss: 0.09120566907681917\n",
            "Epoch: 354, Loss: 0.09034856153946173\n",
            "Epoch: 355, Loss: 0.08955709440143485\n",
            "Epoch: 356, Loss: 0.08877004055600417\n",
            "Epoch: 357, Loss: 0.08793059677669876\n",
            "Epoch: 358, Loss: 0.08720984388338893\n",
            "Epoch: 359, Loss: 0.08635379305403483\n",
            "Epoch: 360, Loss: 0.08563194629785262\n",
            "Epoch: 361, Loss: 0.08483619644845787\n",
            "Epoch: 362, Loss: 0.0840582127045644\n",
            "Epoch: 363, Loss: 0.08329115337447117\n",
            "Epoch: 364, Loss: 0.08258006172744851\n",
            "Epoch: 365, Loss: 0.08182456942373201\n",
            "Epoch: 366, Loss: 0.08105225351295973\n",
            "Epoch: 367, Loss: 0.08035193795436307\n",
            "Epoch: 368, Loss: 0.07959702395294842\n",
            "Epoch: 369, Loss: 0.07888606543603696\n",
            "Epoch: 370, Loss: 0.07815549110895709\n",
            "Epoch: 371, Loss: 0.07747490292316989\n",
            "Epoch: 372, Loss: 0.07673484498732969\n",
            "Epoch: 373, Loss: 0.0760549527445906\n",
            "Epoch: 374, Loss: 0.07537412359134148\n",
            "Epoch: 375, Loss: 0.07464423726655935\n",
            "Epoch: 376, Loss: 0.07398064954108313\n",
            "Epoch: 377, Loss: 0.07334236673226482\n",
            "Epoch: 378, Loss: 0.07263291833039962\n",
            "Epoch: 379, Loss: 0.07196899091726855\n",
            "Epoch: 380, Loss: 0.07132288215583876\n",
            "Epoch: 381, Loss: 0.07068146196635146\n",
            "Epoch: 382, Loss: 0.07002776527875348\n",
            "Epoch: 383, Loss: 0.06936077263794448\n",
            "Epoch: 384, Loss: 0.06873144238795105\n",
            "Epoch: 385, Loss: 0.06810744851827621\n",
            "Epoch: 386, Loss: 0.0674707094501508\n",
            "Epoch: 387, Loss: 0.06686030004761721\n",
            "Epoch: 388, Loss: 0.06625176672088473\n",
            "Epoch: 389, Loss: 0.06561883735029321\n",
            "Epoch: 390, Loss: 0.06502947181855377\n",
            "Epoch: 391, Loss: 0.064420028344581\n",
            "Epoch: 392, Loss: 0.06381817358104806\n",
            "Epoch: 393, Loss: 0.06324122620648459\n",
            "Epoch: 394, Loss: 0.06264415353928741\n",
            "Epoch: 395, Loss: 0.06206282041966915\n",
            "Epoch: 396, Loss: 0.06150785912024347\n",
            "Epoch: 397, Loss: 0.060937823237557164\n",
            "Epoch: 398, Loss: 0.06036130936914369\n",
            "Epoch: 399, Loss: 0.05978952024720217\n",
            "Epoch: 400, Loss: 0.059284795565824756\n",
            "Epoch: 401, Loss: 0.05869053666921038\n",
            "Epoch: 402, Loss: 0.05817247936992269\n",
            "Epoch: 403, Loss: 0.05760566556924268\n",
            "Epoch: 404, Loss: 0.057066667334813824\n",
            "Epoch: 405, Loss: 0.056539815311369146\n",
            "Epoch: 406, Loss: 0.05603300652613765\n",
            "Epoch: 407, Loss: 0.05550144632395945\n",
            "Epoch: 408, Loss: 0.05497097900431407\n",
            "Epoch: 409, Loss: 0.054476266922919375\n",
            "Epoch: 410, Loss: 0.05397415827763708\n",
            "Epoch: 411, Loss: 0.05344465590621296\n",
            "Epoch: 412, Loss: 0.052957397836603616\n",
            "Epoch: 413, Loss: 0.05247573044739271\n",
            "Epoch: 414, Loss: 0.05196759016498139\n",
            "Epoch: 415, Loss: 0.05148857067290105\n",
            "Epoch: 416, Loss: 0.05098579392621392\n",
            "Epoch: 417, Loss: 0.050524293003897915\n",
            "Epoch: 418, Loss: 0.050041118342625465\n",
            "Epoch: 419, Loss: 0.04956251823980557\n",
            "Epoch: 420, Loss: 0.04908474319075283\n",
            "Epoch: 421, Loss: 0.048646285345679836\n",
            "Epoch: 422, Loss: 0.048175854902518425\n",
            "Epoch: 423, Loss: 0.047717075598867315\n",
            "Epoch: 424, Loss: 0.04724851142811148\n",
            "Epoch: 425, Loss: 0.04682878591120243\n",
            "Epoch: 426, Loss: 0.04637859349972323\n",
            "Epoch: 427, Loss: 0.04594856819235965\n",
            "Epoch: 428, Loss: 0.04548879845165893\n",
            "Epoch: 429, Loss: 0.04509660332022529\n",
            "Epoch: 430, Loss: 0.04464169503434708\n",
            "Epoch: 431, Loss: 0.04421584744398531\n",
            "Epoch: 432, Loss: 0.043809971183930575\n",
            "Epoch: 433, Loss: 0.04339146927783364\n",
            "Epoch: 434, Loss: 0.04297587231389786\n",
            "Epoch: 435, Loss: 0.042561555771451244\n",
            "Epoch: 436, Loss: 0.04215640746253101\n",
            "Epoch: 437, Loss: 0.041774112241048565\n",
            "Epoch: 438, Loss: 0.04135515898662178\n",
            "Epoch: 439, Loss: 0.04096032943772642\n",
            "Epoch: 440, Loss: 0.04058109135612061\n",
            "Epoch: 441, Loss: 0.04017551272715393\n",
            "Epoch: 442, Loss: 0.03978856295151146\n",
            "Epoch: 443, Loss: 0.03941302060296661\n",
            "Epoch: 444, Loss: 0.03905503598875121\n",
            "Epoch: 445, Loss: 0.038669388006000144\n",
            "Epoch: 446, Loss: 0.0382876930954425\n",
            "Epoch: 447, Loss: 0.03791766471572613\n",
            "Epoch: 448, Loss: 0.03756101502987899\n",
            "Epoch: 449, Loss: 0.03721087822984708\n",
            "Epoch: 450, Loss: 0.03684572105933177\n",
            "Epoch: 451, Loss: 0.03650932195351312\n",
            "Epoch: 452, Loss: 0.03614231792131537\n",
            "Epoch: 453, Loss: 0.03579887860503636\n",
            "Epoch: 454, Loss: 0.03546365580864643\n",
            "Epoch: 455, Loss: 0.03513360842082061\n",
            "Epoch: 456, Loss: 0.034773024877435284\n",
            "Epoch: 457, Loss: 0.034457747187269364\n",
            "Epoch: 458, Loss: 0.03412978076621106\n",
            "Epoch: 459, Loss: 0.03379461125127579\n",
            "Epoch: 460, Loss: 0.03347438034650527\n",
            "Epoch: 461, Loss: 0.03314654450667532\n",
            "Epoch: 462, Loss: 0.03283579407357856\n",
            "Epoch: 463, Loss: 0.0325164129271319\n",
            "Epoch: 464, Loss: 0.0322121240590748\n",
            "Epoch: 465, Loss: 0.031896973715016715\n",
            "Epoch: 466, Loss: 0.03158988225224771\n",
            "Epoch: 467, Loss: 0.03127961821461979\n",
            "Epoch: 468, Loss: 0.030997705988978084\n",
            "Epoch: 469, Loss: 0.03069205567436783\n",
            "Epoch: 470, Loss: 0.030402356954781634\n",
            "Epoch: 471, Loss: 0.030099397378140373\n",
            "Epoch: 472, Loss: 0.029818931986626825\n",
            "Epoch: 473, Loss: 0.02953676413744688\n",
            "Epoch: 474, Loss: 0.029239813797175884\n",
            "Epoch: 475, Loss: 0.028961212570337874\n",
            "Epoch: 476, Loss: 0.028692763200715968\n",
            "Epoch: 477, Loss: 0.02841658973576207\n",
            "Epoch: 478, Loss: 0.028135959402118858\n",
            "Epoch: 479, Loss: 0.027870075924223976\n",
            "Epoch: 480, Loss: 0.027603252692834326\n",
            "Epoch: 481, Loss: 0.027338559347155848\n",
            "Epoch: 482, Loss: 0.027073501364180918\n",
            "Epoch: 483, Loss: 0.026810822959401105\n",
            "Epoch: 484, Loss: 0.02655361298667757\n",
            "Epoch: 485, Loss: 0.026307124546483943\n",
            "Epoch: 486, Loss: 0.02606451893715482\n",
            "Epoch: 487, Loss: 0.025797030986531785\n",
            "Epoch: 488, Loss: 0.02555841526114627\n",
            "Epoch: 489, Loss: 0.02531893448413987\n",
            "Epoch: 490, Loss: 0.025072111208972177\n",
            "Epoch: 491, Loss: 0.02481945094309355\n",
            "Epoch: 492, Loss: 0.0245929538811508\n",
            "Epoch: 493, Loss: 0.02435655524267962\n",
            "Epoch: 494, Loss: 0.024106231665140705\n",
            "Epoch: 495, Loss: 0.02388216035538598\n",
            "Epoch: 496, Loss: 0.02366337591880246\n",
            "Epoch: 497, Loss: 0.023428521241600577\n",
            "Epoch: 498, Loss: 0.023193454100309235\n",
            "Epoch: 499, Loss: 0.022981738304033092\n",
            "Epoch: 500, Loss: 0.022759210122259038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NX1N2OcBUCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_outputs = []\n",
        "for i, (X, y) in enumerate(test_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    X = X.to(device)\n",
        "    output = model(X)   \n",
        "    test_outputs.append(output.cpu().detach().numpy())\n",
        "test_outputs_converted = []\n",
        "for i in range(len(test_outputs)):\n",
        "    current = test_outputs[i][0]\n",
        "    add = [1 if x >= 0.50 else 0 for x in current]\n",
        "    test_outputs_converted.append(add)\n",
        "test_outputs_converted = np.array(test_outputs_converted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dwAotu9Bc4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc34620b-ba83-469f-9ee2-792875d2f182"
      },
      "source": [
        "accuracy = 0\n",
        "missed = []\n",
        "for i in range(len(test_outputs_converted)):\n",
        "  if collections.Counter(test_outputs_converted[i]) == collections.Counter(converted_test[i]):\n",
        "    accuracy += 1\n",
        "  else:\n",
        "    missed.append(i)\n",
        "print(f'Testing Accuracy: {accuracy/33}')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.7272727272727273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxmhAL8oF3sw",
        "colab_type": "text"
      },
      "source": [
        "# Which genes did the supervised methods identify as the most important?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMeRhTO1G0OM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "87497a39-95a4-4679-aa96-af06282b8e03"
      },
      "source": [
        "original_gene_data = pd.read_csv('expression_data.csv')\n",
        "original_gene_data.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gene_Name</th>\n",
              "      <th>CGGA_1001</th>\n",
              "      <th>CGGA_1006</th>\n",
              "      <th>CGGA_1007</th>\n",
              "      <th>CGGA_1011</th>\n",
              "      <th>CGGA_1015</th>\n",
              "      <th>CGGA_1019</th>\n",
              "      <th>CGGA_1022</th>\n",
              "      <th>CGGA_1023</th>\n",
              "      <th>CGGA_1024</th>\n",
              "      <th>CGGA_1026</th>\n",
              "      <th>CGGA_1034</th>\n",
              "      <th>CGGA_1035</th>\n",
              "      <th>CGGA_1039</th>\n",
              "      <th>CGGA_1045</th>\n",
              "      <th>CGGA_1049</th>\n",
              "      <th>CGGA_1052</th>\n",
              "      <th>CGGA_1072</th>\n",
              "      <th>CGGA_1077</th>\n",
              "      <th>CGGA_1078</th>\n",
              "      <th>CGGA_1079</th>\n",
              "      <th>CGGA_1081</th>\n",
              "      <th>CGGA_1083</th>\n",
              "      <th>CGGA_1091</th>\n",
              "      <th>CGGA_1109</th>\n",
              "      <th>CGGA_1124</th>\n",
              "      <th>CGGA_1139</th>\n",
              "      <th>CGGA_1140</th>\n",
              "      <th>CGGA_1145</th>\n",
              "      <th>CGGA_1171</th>\n",
              "      <th>CGGA_1177</th>\n",
              "      <th>CGGA_1180</th>\n",
              "      <th>CGGA_1188</th>\n",
              "      <th>CGGA_1214</th>\n",
              "      <th>CGGA_1216</th>\n",
              "      <th>CGGA_1218</th>\n",
              "      <th>CGGA_1224</th>\n",
              "      <th>CGGA_1234</th>\n",
              "      <th>CGGA_1237</th>\n",
              "      <th>CGGA_1240</th>\n",
              "      <th>...</th>\n",
              "      <th>CGGA_D39</th>\n",
              "      <th>CGGA_D40</th>\n",
              "      <th>CGGA_D45</th>\n",
              "      <th>CGGA_D46</th>\n",
              "      <th>CGGA_D47</th>\n",
              "      <th>CGGA_D48</th>\n",
              "      <th>CGGA_D52</th>\n",
              "      <th>CGGA_D56</th>\n",
              "      <th>CGGA_D59</th>\n",
              "      <th>CGGA_277</th>\n",
              "      <th>CGGA_518</th>\n",
              "      <th>CGGA_J023</th>\n",
              "      <th>CGGA_J024</th>\n",
              "      <th>CGGA_J042</th>\n",
              "      <th>CGGA_J100</th>\n",
              "      <th>CGGA_1004</th>\n",
              "      <th>CGGA_1008</th>\n",
              "      <th>CGGA_1053</th>\n",
              "      <th>CGGA_1059</th>\n",
              "      <th>CGGA_1070</th>\n",
              "      <th>CGGA_1071</th>\n",
              "      <th>CGGA_1073</th>\n",
              "      <th>CGGA_1074</th>\n",
              "      <th>CGGA_1095</th>\n",
              "      <th>CGGA_1099</th>\n",
              "      <th>CGGA_1114</th>\n",
              "      <th>CGGA_1116</th>\n",
              "      <th>CGGA_1118</th>\n",
              "      <th>CGGA_1119</th>\n",
              "      <th>CGGA_1197</th>\n",
              "      <th>CGGA_1246</th>\n",
              "      <th>CGGA_1275</th>\n",
              "      <th>CGGA_1450</th>\n",
              "      <th>CGGA_1460</th>\n",
              "      <th>CGGA_1475</th>\n",
              "      <th>CGGA_243</th>\n",
              "      <th>CGGA_247</th>\n",
              "      <th>CGGA_738</th>\n",
              "      <th>CGGA_759</th>\n",
              "      <th>CGGA_D30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1BG</td>\n",
              "      <td>12.64</td>\n",
              "      <td>7.03</td>\n",
              "      <td>30.09</td>\n",
              "      <td>6.64</td>\n",
              "      <td>1.83</td>\n",
              "      <td>39.03</td>\n",
              "      <td>12.96</td>\n",
              "      <td>9.28</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.56</td>\n",
              "      <td>3.18</td>\n",
              "      <td>49.16</td>\n",
              "      <td>9.99</td>\n",
              "      <td>40.35</td>\n",
              "      <td>14.70</td>\n",
              "      <td>4.48</td>\n",
              "      <td>11.04</td>\n",
              "      <td>48.61</td>\n",
              "      <td>13.12</td>\n",
              "      <td>9.59</td>\n",
              "      <td>30.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>47.06</td>\n",
              "      <td>21.99</td>\n",
              "      <td>10.63</td>\n",
              "      <td>18.90</td>\n",
              "      <td>23.58</td>\n",
              "      <td>9.62</td>\n",
              "      <td>11.87</td>\n",
              "      <td>38.88</td>\n",
              "      <td>55.84</td>\n",
              "      <td>23.07</td>\n",
              "      <td>18.49</td>\n",
              "      <td>18.82</td>\n",
              "      <td>20.02</td>\n",
              "      <td>21.39</td>\n",
              "      <td>6.78</td>\n",
              "      <td>4.97</td>\n",
              "      <td>10.89</td>\n",
              "      <td>...</td>\n",
              "      <td>10.13</td>\n",
              "      <td>17.68</td>\n",
              "      <td>8.97</td>\n",
              "      <td>16.73</td>\n",
              "      <td>9.92</td>\n",
              "      <td>28.25</td>\n",
              "      <td>7.06</td>\n",
              "      <td>21.27</td>\n",
              "      <td>20.50</td>\n",
              "      <td>22.14</td>\n",
              "      <td>7.52</td>\n",
              "      <td>9.38</td>\n",
              "      <td>14.35</td>\n",
              "      <td>21.13</td>\n",
              "      <td>2.24</td>\n",
              "      <td>20.74</td>\n",
              "      <td>15.97</td>\n",
              "      <td>26.36</td>\n",
              "      <td>20.84</td>\n",
              "      <td>5.25</td>\n",
              "      <td>11.83</td>\n",
              "      <td>39.87</td>\n",
              "      <td>20.71</td>\n",
              "      <td>10.94</td>\n",
              "      <td>5.74</td>\n",
              "      <td>13.74</td>\n",
              "      <td>14.75</td>\n",
              "      <td>19.61</td>\n",
              "      <td>21.50</td>\n",
              "      <td>14.90</td>\n",
              "      <td>4.46</td>\n",
              "      <td>28.42</td>\n",
              "      <td>19.32</td>\n",
              "      <td>2.24</td>\n",
              "      <td>18.89</td>\n",
              "      <td>4.03</td>\n",
              "      <td>5.29</td>\n",
              "      <td>4.64</td>\n",
              "      <td>7.07</td>\n",
              "      <td>31.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1BG-AS1</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1.13</td>\n",
              "      <td>6.64</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.39</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.25</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.22</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>7.75</td>\n",
              "      <td>1.61</td>\n",
              "      <td>9.49</td>\n",
              "      <td>5.02</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.73</td>\n",
              "      <td>5.59</td>\n",
              "      <td>4.37</td>\n",
              "      <td>2.07</td>\n",
              "      <td>3.78</td>\n",
              "      <td>3.99</td>\n",
              "      <td>5.55</td>\n",
              "      <td>4.27</td>\n",
              "      <td>1.15</td>\n",
              "      <td>2.71</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.88</td>\n",
              "      <td>2.97</td>\n",
              "      <td>6.18</td>\n",
              "      <td>9.10</td>\n",
              "      <td>1.90</td>\n",
              "      <td>4.85</td>\n",
              "      <td>4.01</td>\n",
              "      <td>5.10</td>\n",
              "      <td>3.61</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.98</td>\n",
              "      <td>2.95</td>\n",
              "      <td>...</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.38</td>\n",
              "      <td>4.89</td>\n",
              "      <td>2.56</td>\n",
              "      <td>3.82</td>\n",
              "      <td>1.03</td>\n",
              "      <td>2.51</td>\n",
              "      <td>2.86</td>\n",
              "      <td>5.46</td>\n",
              "      <td>2.62</td>\n",
              "      <td>1.62</td>\n",
              "      <td>2.14</td>\n",
              "      <td>3.48</td>\n",
              "      <td>0.73</td>\n",
              "      <td>2.23</td>\n",
              "      <td>4.52</td>\n",
              "      <td>2.61</td>\n",
              "      <td>4.16</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.04</td>\n",
              "      <td>6.93</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.41</td>\n",
              "      <td>5.29</td>\n",
              "      <td>1.41</td>\n",
              "      <td>3.19</td>\n",
              "      <td>1.97</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.57</td>\n",
              "      <td>8.36</td>\n",
              "      <td>2.21</td>\n",
              "      <td>1.60</td>\n",
              "      <td>5.31</td>\n",
              "      <td>2.47</td>\n",
              "      <td>3.69</td>\n",
              "      <td>0.85</td>\n",
              "      <td>1.99</td>\n",
              "      <td>2.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A2M</td>\n",
              "      <td>452.92</td>\n",
              "      <td>106.54</td>\n",
              "      <td>206.70</td>\n",
              "      <td>707.17</td>\n",
              "      <td>824.32</td>\n",
              "      <td>155.14</td>\n",
              "      <td>538.33</td>\n",
              "      <td>26.39</td>\n",
              "      <td>219.35</td>\n",
              "      <td>302.00</td>\n",
              "      <td>139.67</td>\n",
              "      <td>143.45</td>\n",
              "      <td>79.09</td>\n",
              "      <td>202.67</td>\n",
              "      <td>345.41</td>\n",
              "      <td>117.38</td>\n",
              "      <td>126.56</td>\n",
              "      <td>412.72</td>\n",
              "      <td>170.82</td>\n",
              "      <td>136.63</td>\n",
              "      <td>345.27</td>\n",
              "      <td>763.49</td>\n",
              "      <td>235.32</td>\n",
              "      <td>60.24</td>\n",
              "      <td>561.11</td>\n",
              "      <td>61.97</td>\n",
              "      <td>56.12</td>\n",
              "      <td>483.18</td>\n",
              "      <td>614.56</td>\n",
              "      <td>422.29</td>\n",
              "      <td>151.96</td>\n",
              "      <td>67.55</td>\n",
              "      <td>76.20</td>\n",
              "      <td>78.47</td>\n",
              "      <td>234.72</td>\n",
              "      <td>484.48</td>\n",
              "      <td>903.16</td>\n",
              "      <td>134.09</td>\n",
              "      <td>508.32</td>\n",
              "      <td>...</td>\n",
              "      <td>70.55</td>\n",
              "      <td>67.67</td>\n",
              "      <td>115.30</td>\n",
              "      <td>173.03</td>\n",
              "      <td>182.86</td>\n",
              "      <td>116.36</td>\n",
              "      <td>69.12</td>\n",
              "      <td>183.79</td>\n",
              "      <td>248.62</td>\n",
              "      <td>119.77</td>\n",
              "      <td>51.07</td>\n",
              "      <td>94.13</td>\n",
              "      <td>177.72</td>\n",
              "      <td>255.94</td>\n",
              "      <td>125.46</td>\n",
              "      <td>277.46</td>\n",
              "      <td>260.36</td>\n",
              "      <td>298.05</td>\n",
              "      <td>145.93</td>\n",
              "      <td>256.82</td>\n",
              "      <td>214.55</td>\n",
              "      <td>249.91</td>\n",
              "      <td>452.45</td>\n",
              "      <td>276.66</td>\n",
              "      <td>194.31</td>\n",
              "      <td>384.38</td>\n",
              "      <td>174.69</td>\n",
              "      <td>108.87</td>\n",
              "      <td>65.29</td>\n",
              "      <td>78.92</td>\n",
              "      <td>87.20</td>\n",
              "      <td>243.89</td>\n",
              "      <td>73.80</td>\n",
              "      <td>111.50</td>\n",
              "      <td>340.53</td>\n",
              "      <td>108.06</td>\n",
              "      <td>97.24</td>\n",
              "      <td>160.70</td>\n",
              "      <td>263.66</td>\n",
              "      <td>192.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A2M-AS1</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.63</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.34</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.37</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.81</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1.32</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.93</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.44</td>\n",
              "      <td>...</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.06</td>\n",
              "      <td>3.69</td>\n",
              "      <td>2.32</td>\n",
              "      <td>0.85</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.42</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.48</td>\n",
              "      <td>2.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2ML1</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.33</td>\n",
              "      <td>4.96</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.39</td>\n",
              "      <td>2.34</td>\n",
              "      <td>0.68</td>\n",
              "      <td>2.84</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.94</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.25</td>\n",
              "      <td>4.57</td>\n",
              "      <td>7.31</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.04</td>\n",
              "      <td>9.13</td>\n",
              "      <td>1.01</td>\n",
              "      <td>5.71</td>\n",
              "      <td>0.69</td>\n",
              "      <td>2.28</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.25</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.48</td>\n",
              "      <td>...</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.38</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.04</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.10</td>\n",
              "      <td>4.07</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>0.79</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1.02</td>\n",
              "      <td>2.09</td>\n",
              "      <td>3.03</td>\n",
              "      <td>2.74</td>\n",
              "      <td>2.41</td>\n",
              "      <td>3.34</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.69</td>\n",
              "      <td>2.58</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.55</td>\n",
              "      <td>0.71</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.60</td>\n",
              "      <td>3.86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  326 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gene_Name  CGGA_1001  CGGA_1006  ...  CGGA_738  CGGA_759  CGGA_D30\n",
              "0      A1BG      12.64       7.03  ...      4.64      7.07     31.41\n",
              "1  A1BG-AS1       2.12       1.13  ...      0.85      1.99      2.65\n",
              "2       A2M     452.92     106.54  ...    160.70    263.66    192.54\n",
              "3   A2M-AS1       3.30       0.13  ...      0.38      0.48      2.51\n",
              "4     A2ML1       0.04       0.33  ...      3.33      0.60      3.86\n",
              "\n",
              "[5 rows x 326 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVS2pG5IF-Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gene_names_pi_important_pca = []\n",
        "for i in non_zero_pi:\n",
        "  gene_names_pi_important_pca.append(original_gene_data.iloc[i, 0])\n",
        "\n",
        "gene_names_rfe_important_pca = []\n",
        "for i in top_100_rfe:\n",
        "  gene_names_rfe_important_pca.append(original_gene_data.iloc[i, 0])\n",
        "\n",
        "gene_names_sparse_pi = []\n",
        "for i in non_zero_sparse:\n",
        "  gene_names_sparse_pi.append(original_gene_data.iloc[i, 0])\n",
        "\n",
        "gene_names_sparse_rfe = []\n",
        "for i in top_100_rfe_sparse:\n",
        "  gene_names_sparse_rfe.append(original_gene_data.iloc[i, 0])\n",
        "\n",
        "gene_names_fa_pi = []\n",
        "gene_names_fa_rfe = []\n",
        "\n",
        "for i in non_zero_fa:\n",
        "  gene_names_fa_pi.append(original_gene_data.iloc[i, 0])\n",
        "\n",
        "for i in top_100_rfe_fa:\n",
        "  gene_names_fa_rfe.append(original_gene_data.iloc[i, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMIkeCnW1es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "febc71be-d3ef-42e6-8e9f-a2c2bb834cbf"
      },
      "source": [
        "print('Important genes identified from both feature selection: ')\n",
        "print(gene_names_pi_important_pca)\n",
        "print(gene_names_rfe_important_pca)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Important genes identified from both feature selection: \n",
            "['ATP6AP2', 'ANAPC1P1', 'ABCC11', 'AC005682.6', 'AC009299.5', 'AP003039.3', 'ATAT1', 'BLZF1', 'C9orf139']\n",
            "['AC006126.4', 'ABHD16A', 'ARMCX6', 'AC093627.10', 'C5orf64', 'ATP13A4', 'BTBD1', 'C2orf61', 'ASCL2', 'ACAT2', 'ATIC', 'AC002429.5', 'BTBD17', 'BCKDHA', 'C1GALT1C1', 'ABCB5', 'AC097724.3', 'ADCY1', 'BMS1P11', 'ANKRD54', 'AP003025.2', 'BLZF1', 'ASB13', 'BRD1', 'AC016722.2', 'C9orf96', 'AC004257.3', 'ATP13A4-AS1', 'APOBEC2', 'AC004449.6', 'ARFGAP3', 'BACH1', 'C4orf48', 'AC068134.10', 'BX255923.3', 'ADPRM', 'ARMC7', 'AC007277.3', 'C1orf65', 'AC005104.3', 'C19orf80', 'BICC1', 'ATP2C1', 'ABHD8', 'ACSL6', 'ANAPC1P1', 'ATP5I', 'AC018730.3', 'C18orf21', 'ACO2', 'AC018755.16', 'AADAT', 'CACNG7', 'AC062017.1', 'APEX1', 'ANKFN1', 'C10orf55', 'ACY3', 'AC125232.1', 'BNC2', 'AC138430.4', 'ANGPTL2', 'C9orf171', 'ANKRD24', 'C9orf139', 'AP001053.11', 'ADAM20P1', 'AC016700.3', 'C19orf40', 'ANKUB1', 'BMS1P1', 'A2MP1', 'ALG1L', 'ATP5F1P5', 'AC022498.1', 'AC004166.6', 'AP006621.5', 'ARFIP1', 'ANGPT4', 'C1orf226', 'APOL1', 'AC005618.6', 'ARPP21', 'AC005682.6', 'ABCC11', 'ARHGAP31', 'ADH4', 'AC092653.5', 'C6orf226', 'AR', 'ABCF1', 'AP000251.3', 'BANP', 'ABCG2', 'AC139100.3', 'B4GALT2', 'ADAMTS17', 'BTN3A2', 'ANXA1', 'C14orf37']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}